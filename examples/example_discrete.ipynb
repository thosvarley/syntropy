{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad831f2-d424-4d38-8f6f-249ddf83534b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a31f53-532c-440a-b869-0859468e30c5",
   "metadata": {},
   "source": [
    "## Discrete analysis tutorial.\n",
    "\n",
    "For discrete probability distributions, the basic object of study is a distribution, represented as a dictionary of the form:\n",
    "\n",
    "``(x, y, z, ... w) : P(x,y,z,..w)``\n",
    "\n",
    "The key is a tuple of the state of each element of the system, and the value is the probability of that state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270529db-603c-4d81-ab13-70d9a29ad0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Distribution\n",
      "(0, 0, 0) 0.25\n",
      "(0, 1, 1) 0.25\n",
      "(1, 0, 1) 0.25\n",
      "(1, 1, 0) 0.25\n",
      " \n",
      "Summed Dice Distribution\n",
      "(1, 1, 2) 0.027777777777777776\n",
      "(1, 2, 3) 0.027777777777777776\n",
      "(1, 3, 4) 0.027777777777777776\n",
      "(1, 4, 5) 0.027777777777777776\n",
      "(1, 5, 6) 0.027777777777777776\n",
      "(1, 6, 7) 0.027777777777777776\n",
      "...\n",
      " \n",
      "Trivariate One-Hot Distribution\n",
      "(0, 0, 1) 0.3333333333333333\n",
      "(0, 1, 0) 0.3333333333333333\n",
      "(1, 0, 0) 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# The basic object for the discrete PyGID package is a distribution. ,..z\n",
    "# I've included a bunch of stock distributions in a module. \n",
    "\n",
    "from syntropy.discrete.distributions import XOR_DIST, GIANT_BIT, SUMMED_DICE, ONE_HOT_3_DIST\n",
    "\n",
    "print(\"XOR Distribution\")\n",
    "for key in XOR_DIST.keys():\n",
    "    print(key, XOR_DIST[key])\n",
    "print(\" \")\n",
    "print(\"Summed Dice Distribution\")\n",
    "for key in SUMMED_DICE.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, SUMMED_DICE[key])\n",
    "print(\"...\")\n",
    "print(\" \")\n",
    "print(\"Trivariate One-Hot Distribution\")\n",
    "for key in ONE_HOT_3_DIST:\n",
    "    print(key, ONE_HOT_3_DIST[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cb855-9bb9-49b8-8b09-a69df2e4f364",
   "metadata": {},
   "source": [
    "## Basic Shannon measures\n",
    "\n",
    "The basic object of study in discrete mutual information theory is the *Shannon entropy* of a probability distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X) = -\\sum_{x\\in\\mathcal{X}}P(x)\\log P(x)\n",
    "\\end{equation}\n",
    "\n",
    "The entropy quantifies, on average, how uncertain are you about the outcome of a random draw from the distribution $P(X)$. The entropy is an expected value over a distribution of \"local\" entropies:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X) = \\mathbb{E}\\big[h(x)\\big]\n",
    "\\end{equation}\n",
    "\n",
    "Where $h(x) = -\\log P(x)$.\n",
    "\n",
    "For a set of variables you can compute the joint entropy:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X,Y) = -\\sum_{x,y \\in \\mathcal{X}\\times\\mathcal{Y}}P(x,y)\\log P(x,y)\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mathcal{X}\\times\\mathcal{Y}$ is the Cartesian product of the support sets of $X$ and $Y$. The local entropy again is computed as $h(x,y) = -\\log P(x,y)$.\n",
    "\n",
    "There is also a conditional entropy, which quantifies the updated uncertainty about the state of $X$ after the state of $Y$ is known. \n",
    "\n",
    "\\begin{align}\n",
    "    H(X|Y) &= H(X,Y) - H(Y) \\\\ \n",
    "    &= -\\sum_{x,y\\in\\mathcal{X}\\times\\mathcal{Y}}P(x,y)\\log P(x|y)\n",
    "\\end{align}\n",
    "\n",
    "Note that the expectation is computed with respect to the joint distribution $P(x,y)$, but the local conditional entropy is the expected $h(x|y) = -\\log P(x|y)$.\n",
    "\n",
    "Finally, there is the mutual information, which quantifies how much learning the state of one variable reduces our uncertainty about the state of another:\n",
    "\n",
    "\\begin{align}\n",
    "    I(X;Y) &= H(X) - H(X|Y) \\\\\n",
    "    &= \\sum_{x,y\\in\\mathcal{X}\\times\\mathcal{Y}}P(x,y)\\log\\frac{P(x|y)}{P(x)}\n",
    "\\end{align}\n",
    "\n",
    "Note that mutual information is symmetric in it's arguments: $I(X;Y)=I(Y;X)$. The mutual information can also be thought of as a measure of the amount of information gained when updating from a *prior* belief that $X$ and $Y$ are independent to the true distirbution $P(X,Y)$. \n",
    "\n",
    "Like the entropy, the mutual information has a local form:\n",
    "\n",
    "\\begin{align}\n",
    "I(X;Y) = \\mathbb{E}\\bigg[\\log \\frac{P(x|y)}{P(x)}\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "This local mutual information can be negative, unlike the expected mutual information, which is strictly non-negative by Jensen's Inequality. \n",
    "\n",
    "We can also compute a conditional mutual information, in the same way we can compute a conditional entropy:\n",
    " \n",
    "\\begin{align}\n",
    "    I(X;Y|Z) = H(X|Z) + H(X|Y,Z)\n",
    "\\end{align}\n",
    "\n",
    "The condiitonal mutual information quantifies how much learning the state of $Y$ reduces our uncertainty about the state of $X$ given that we have already learned the state of $Z$. If $I(X;Y|Z)>I(X;Y)$, then there is dependency between $X$ and $Y$ that only becomes \"illuminated\" when $X$, $Y$, and $Z$ are considered together, while if $I(X;Y|Z) < I(X;Y)$, the there is redundant information shared by $X$ and $Y$ that is \"conditioned out\" when adding $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2e48e8-f00f-4ca7-a0e2-36b7f572cc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointwise entropies:\n",
      "(0, 0, 1) 1.5849625007211563 bit\n",
      "(0, 1, 0) 1.5849625007211563 bit\n",
      "(1, 0, 0) 1.5849625007211563 bit\n",
      "Expected entropy: 1.584962500721156 bit\n",
      " \n",
      "Pointwise entropies:\n",
      "(0, 1) 1.5849625007211563 bit\n",
      "(1, 0) 1.5849625007211563 bit\n",
      "(0, 0) 1.5849625007211563 bit\n",
      "Expected entropy: 1.584962500721156 bit\n",
      "\n",
      "Pointwise conditional entropies:\n",
      "((0,), (1,)) 1.0 bit\n",
      "((1,), (0,)) 1.0 bit\n",
      "((1,), (1,)) 1.0 bit\n",
      "((0,), (0,)) 1.0 bit\n",
      "Expected entropy: 1.0 bit\n",
      "\n",
      "Pointwise conditional MIs:\n",
      "((0,), (0,), (0,)) 1.0 bit\n",
      "((1,), (0,), (1,)) 1.0 bit\n",
      "((1,), (1,), (0,)) 1.0 bit\n",
      "((0,), (1,), (1,)) 1.0 bit\n",
      "Expected MI: 1.0 bit\n",
      "\n",
      "Pointwise conditional MIs:\n",
      "((0,), (0,), (0,)) 0.0 bit\n",
      "((1,), (1,), (1,)) 0.0 bit\n",
      "Expected MI: 0.0 bit\n"
     ]
    }
   ],
   "source": [
    "from syntropy.discrete.shannon import shannon_entropy, kullback_leibler_divergence, mutual_information, conditional_entropy, conditional_mutual_information\n",
    "from syntropy.discrete.utils import get_marginal_distribution\n",
    "\n",
    "# You can compute basic entropy functions\n",
    "h_ptw, h_avg = shannon_entropy(ONE_HOT_3_DIST)\n",
    "\n",
    "# Generally the the measures return a dictory of pointwise/local values and an expected value. \n",
    "print(\"Pointwise entropies:\")\n",
    "for key in h_ptw.keys():\n",
    "    print(key, h_ptw[key], \"bit\")\n",
    "print(f\"Expected entropy: {h_avg} bit\")\n",
    "print(\" \")\n",
    "\n",
    "# If you want just the entropy of a subset of elements, you marginalize the distribution.\n",
    "h_ptw, h_avg = shannon_entropy(\n",
    "    get_marginal_distribution((0,1), ONE_HOT_3_DIST)\n",
    ")\n",
    "print(\"Pointwise entropies:\")\n",
    "for key in h_ptw.keys():\n",
    "    print(key, h_ptw[key], \"bit\")\n",
    "print(f\"Expected entropy: {h_avg} bit\")\n",
    "\n",
    "# You can do conditional entropy as well:\n",
    "h_ptw, h_avg = conditional_entropy((0,),(1,), XOR_DIST)\n",
    "\n",
    "print(\"\") \n",
    "print(\"Pointwise conditional entropies:\")\n",
    "for key in h_ptw.keys():\n",
    "    print(key, h_ptw[key], \"bit\")\n",
    "print(f\"Expected entropy: {h_avg} bit\")\n",
    "# In this ptw dict, the residual (X) and conditioning (Y) are separate tuples. \n",
    "# Since it's an XOR, knowing the state of one reduces no uncertainty about the other. \n",
    "\n",
    "# We can do a similar thing with the conditional mutual information\n",
    "i_ptw, i_avg = conditional_mutual_information((0,), (1,), (2,), XOR_DIST)\n",
    "# Now the MI between (0,) and (1,) is 1 bit, since knowing the state of (2,) resolves the system.\n",
    "print(\"\") \n",
    "print(\"Pointwise conditional MIs:\")\n",
    "for key in i_ptw.keys():\n",
    "    print(key, i_ptw[key], \"bit\")\n",
    "print(f\"Expected MI: {i_avg} bit\")\n",
    "\n",
    "# Compare to the redundant, GIANT_BIT distribution:\n",
    "i_ptw, i_avg = conditional_mutual_information((0,), (1,), (2,), GIANT_BIT)\n",
    "# Now the MI between (0,) and (1,) is 1 bit, since knowing the state of (2,) resolves the system.\n",
    "print(\"\") \n",
    "print(\"Pointwise conditional MIs:\")\n",
    "for key in i_ptw.keys():\n",
    "    print(key, i_ptw[key], \"bit\")\n",
    "print(f\"Expected MI: {i_avg} bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82381fb7-3224-4be0-83aa-e51273e3a735",
   "metadata": {},
   "source": [
    "## Multivariate information measures\n",
    "\n",
    "The mutual information describes the dependency between two variables, potentially conditioned on a third. When studying complex systems, however, we are often interested in the interaction between many variables simultaniously. To that, several generalizaitons of the mutual information have been proposed. \n",
    "\n",
    "The first is the total correlation:\n",
    "\n",
    "\\begin{align}\n",
    "    TC(\\textbf{X}) = \\sum_{i=1}^{N}H(X_i) - H(\\textbf{X})\n",
    "\\end{align}\n",
    "\n",
    "The total correlation can be thought of as a measure of synchrony - it is maximized when every element $X_i$ is a copy of every other $X_j$. \n",
    "\n",
    "The next measure is the dual total correlation:\n",
    "\n",
    "\\begin{align}\n",
    "    DTC(\\textbf{X}) = H(\\textbf{X}) - \\sum_{i=1}^{N}H(X_i|\\textbf{X}^{-i})\n",
    "\\end{align}\n",
    "\n",
    "Where $\\textbf{X}^{-i}$ is the joint state of every element of $\\textbf{X}$ excluding $X_i$. \n",
    "\n",
    "Unlike the total correlation, which is maximized by synchrony, the dual total correlation is maximized by complexity: specifically, complex patterns of information-sharing. \n",
    "\n",
    "The third measure, the co-information is rarely used, as it ceases to be meaningful above three variables. It is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "    Co(\\textbf{X}) = \\sum_{y\\subseteq\\{N\\}}-1^{|y|}H(\\textbf{X}^{y})\n",
    "\\end{align}\n",
    "\n",
    "Unlike the total and dual total correlations, the co-information can be negative. \n",
    "\n",
    "Two more measures of multivariate information can be constructed from the total correlation and dual total correlation. The first is the S-information:\n",
    "\n",
    "\\begin{align}\n",
    "    \\Sigma(\\textbf{X}) &= TC(\\textbf{X}) DTC(\\textbf{X}) \\\\\n",
    "    &= \\sum_{i=1}^{N}I(X_i;\\textbf{X}^{-i})\n",
    "\\end{align}\n",
    "\n",
    "Described by James and Crutchfield as the \"very-mutual information\", the S-information quantifies the total amount of dependency each element participates in. The second measure, the O-information is given by:\n",
    "\n",
    "\\begin{align}\n",
    "    \\Omega(\\textbf{X}) = TC(\\textbf{X}) - DTC(\\textbf{X})\n",
    "\\end{align}\n",
    "\n",
    "The O-information can be negative, in which case there is more information in the joint state $\\textbf{X}$ then in all of the leave-one-out marginals $\\textbf{X}^{-i}$. We can see this by writing O-information in the equivalent form:\n",
    "\n",
    "\\begin{align}\n",
    "    \\Omega(\\textbf{X}) = -\\bigg[(N-2)TC(\\textbf{X}) - \\sum_{i=1}^{N}TC(\\textbf{X}^{-i})\\bigg]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3fddffa-9801-49ae-b877-580a2cf953a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pointwise TC(SUMMED DICE):\n",
      "(1, 1, 2) 5.169925001442312 bit\n",
      "(1, 2, 3) 4.169925001442312 bit\n",
      "(1, 3, 4) 3.584962500721156 bit\n",
      "(1, 4, 5) 3.169925001442312 bit\n",
      "(1, 5, 6) 2.84799690655495 bit\n",
      "(1, 6, 7) 2.584962500721156 bit\n",
      "...\n",
      "Expected TC(SUMMED DICE): 3.27440191928877 bit\n",
      "\n",
      "Pointwise DTC(SUMMED DICE):\n",
      "(1, 1, 2) 5.169925001442312 bit\n",
      "(1, 2, 3) 5.169925001442312 bit\n",
      "(1, 3, 4) 5.169925001442312 bit\n",
      "(1, 4, 5) 5.169925001442312 bit\n",
      "(1, 5, 6) 5.169925001442312 bit\n",
      "(1, 6, 7) 5.169925001442312 bit\n",
      "...\n",
      "Expected DTC(SUMMED DICE): 5.16992500144231 bit\n",
      "\n",
      "Pointwise O(SUMMED DICE):\n",
      "(1, 1, 2) -0.0 bit\n",
      "(1, 2, 3) -1.0 bit\n",
      "(1, 3, 4) -1.584962500721156 bit\n",
      "(1, 4, 5) -1.9999999999999998 bit\n",
      "(1, 5, 6) -2.3219280948873626 bit\n",
      "(1, 6, 7) -2.584962500721156 bit\n",
      "...\n",
      "Expected O(SUMMED DICE): -1.8955230821535407 bit\n",
      "\n",
      "Pointwise S(SUMMED DICE):\n",
      "(1, 1, 2) 10.339850002884624 bit\n",
      "(1, 2, 3) 9.339850002884624 bit\n",
      "(1, 3, 4) 8.754887502163468 bit\n",
      "(1, 4, 5) 8.339850002884624 bit\n",
      "(1, 5, 6) 8.017921907997263 bit\n",
      "(1, 6, 7) 7.754887502163468 bit\n",
      "...\n",
      "Expected S(SUMMED DICE): 8.444326920731081 bit\n",
      "\n",
      "Pointwise Co(SUMMED DICE):\n",
      "(1, 1, 2) 0.0 bit\n",
      "(1, 2, 3) -1.0 bit\n",
      "(1, 3, 4) -1.584962500721156 bit\n",
      "(1, 4, 5) -2.0 bit\n",
      "(1, 5, 6) -2.3219280948873617 bit\n",
      "(1, 6, 7) -2.584962500721156 bit\n",
      "...\n",
      "Expected Co(SUMMED DICE): -1.895523082153541 bit\n"
     ]
    }
   ],
   "source": [
    "from syntropy.discrete.multivariate_mi import total_correlation, dual_total_correlation, s_information, o_information, co_information\n",
    "\n",
    "# We can compute a variety of statistics on a given distribution\n",
    "ptw, tc = total_correlation(SUMMED_DICE)\n",
    "print(\"\") \n",
    "print(\"Pointwise TC(SUMMED DICE):\")\n",
    "for key in ptw.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, ptw[key], \"bit\")\n",
    "print(\"...\")\n",
    "print(f\"Expected TC(SUMMED DICE): {tc} bit\")\n",
    "\n",
    "ptw, dtc = dual_total_correlation(SUMMED_DICE)\n",
    "print(\"\") \n",
    "print(\"Pointwise DTC(SUMMED DICE):\")\n",
    "for key in ptw.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, ptw[key], \"bit\")\n",
    "print(\"...\")\n",
    "print(f\"Expected DTC(SUMMED DICE): {dtc} bit\")\n",
    "\n",
    "\n",
    "ptw, o = o_information(SUMMED_DICE)\n",
    "print(\"\") \n",
    "print(\"Pointwise O(SUMMED DICE):\")\n",
    "for key in ptw.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, ptw[key], \"bit\")\n",
    "print(\"...\")\n",
    "print(f\"Expected O(SUMMED DICE): {o} bit\")\n",
    "\n",
    "ptw, s = s_information(SUMMED_DICE)\n",
    "print(\"\") \n",
    "print(\"Pointwise S(SUMMED DICE):\")\n",
    "for key in ptw.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, ptw[key], \"bit\")\n",
    "print(\"...\")\n",
    "print(f\"Expected S(SUMMED DICE): {s} bit\")\n",
    "\n",
    "# For three variables the co-information is the same as the o-information\n",
    "# but this is not true for N > 3.\n",
    "ptw, co = co_information(SUMMED_DICE)\n",
    "print(\"\") \n",
    "print(\"Pointwise Co(SUMMED DICE):\")\n",
    "for key in ptw.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, ptw[key], \"bit\")\n",
    "print(\"...\")\n",
    "print(f\"Expected Co(SUMMED DICE): {co} bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de60aa-2519-4dde-8f75-56a30c51ff34",
   "metadata": {},
   "source": [
    "## Marginalizing distributions\n",
    "\n",
    "Syntropy has some handy tools for maginalizing distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132e502b-1b4a-4d92-a997-536039bfe576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maginalizing out X_0\n",
      "(1, 6) 0.027777777777777776\n",
      "(1, 3) 0.027777777777777776\n",
      "(1, 2) 0.027777777777777776\n",
      "(1, 5) 0.027777777777777776\n",
      "(1, 4) 0.027777777777777776\n",
      "(1, 7) 0.027777777777777776\n",
      "...\n",
      "Keeping X_1 and X_2\n",
      " \n",
      "(1, 6) 0.027777777777777776\n",
      "(1, 3) 0.027777777777777776\n",
      "(1, 2) 0.027777777777777776\n",
      "(1, 5) 0.027777777777777776\n",
      "(1, 4) 0.027777777777777776\n",
      "(1, 7) 0.027777777777777776\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# If you want to marginalize a distribution \n",
    "from syntropy.discrete.utils import marginalize_out, get_marginal_distribution\n",
    "\n",
    "# If you want to marginalize out a variable:\n",
    "print(\"Maginalizing out X_0\")\n",
    "marg_out = marginalize_out((0,), SUMMED_DICE)\n",
    "for key in marg_out.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, marg_out[key])\n",
    "print(\"...\")\n",
    "# And if you want to marginalize out everything but a given variable \n",
    "\n",
    "print(\"Keeping X_1 and X_2\")\n",
    "marg = get_marginal_distribution((1,2), SUMMED_DICE)\n",
    "print(\" \")\n",
    "for key in marg.keys():\n",
    "    if key[0] == 1:\n",
    "        print(key, marg[key])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9627c-916b-47de-8e7a-e76b125818be",
   "metadata": {},
   "source": [
    "## Information decomposition\n",
    "\n",
    "The final set of basic tools for analyzing joint distributions is the partial information decomposition (PID) and its derivatives (the partial entropy decomposition (PED) and the generalized information decomposition (GID), all of which inter-relate). \n",
    "\n",
    "The PID takes the mutual information that a set of inputs disclose about a shared target and breaks it down into redundant, unique, and synergistic parts. \n",
    "\n",
    "\\begin{align}\n",
    "    I(X_1,X_2;Y) = I_\\partial^{12Y}(\\{X_1\\}\\{X_2\\};Y) + I_\\partial^{12Y}(\\{X_1\\};Y) + I_\\partial^{12Y}(\\{X_2\\};Y) + I_\\partial^{12Y}(\\{X_1,X_2\\};Y)\n",
    "\\end{align}\n",
    "\n",
    "$I_\\partial^{12Y}(\\{X_1\\}\\{X_2\\};Y)$ represents the information about $Y$ that could be learned by observing $X_1$ alone or $X_2$ alone (the information redundnatly shared by the two $X_i$. $I_\\partial^{12Y}(\\{X_i\\};Y)$ is the information about $Y$ that can only be learned by observing $X_i$ alone, and $I_\\partial^{12Y}(\\{X_1,X_2\\};Y)$ is the synergistic information that can only be learned when $X_1$ and $X_2$ are observed together. \n",
    "\n",
    "By leveraging the identity that $I(X_1,X_2...X_k;\\textbf{X}) = H(\\textbf{X})$, we can also produce a partial entropy decomposition, which breaks down the total amount of information that the *parts* disclose about the *whole* of which they are a part. The result is a decomposition of the entropy rather than the mutual information.\n",
    "\n",
    "\\begin{align}\n",
    "    H(X_1,X_2) = H_\\partial^{12}(\\{X_1\\}\\{X_2\\}) + H_\\partial^{12}(\\{X_1\\}) + I_\\partial^{12}(\\{X_2\\}) + H_\\partial^{12}(\\{X_1,X_2\\})\n",
    "\\end{align}\n",
    "\n",
    "All information decompositions require defining a redundancy function, which is then used to bootstrap the higher-order synergies. While many functions have been proposed, syntropy includes a subset that are designed specifically to operate on local, as well as expected entropies and mutual informations. For details, see the documentation.\n",
    "\n",
    "The final decompostion included is the generalized information decomposition, which decomposes the information gained when updating from an arbitrary prior to an arbitrary posterior. For details, see:\n",
    "\n",
    "    Thomas F. Varley (2024), \n",
    "    Generalized Decomposition of Multivariate Information, \n",
    "    PLoS ONE\n",
    "    https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0297128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d506e5c-c6ee-4a40-979f-6b22adc641a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of SUMMED_DICE using the i_sx function\n",
      "((0,),) 0.8744691179161403\n",
      "((0,), (1,)) -0.18502969934852634\n",
      "((1,),) 0.8744691179161403\n",
      "((0, 1),) 1.710493382805015\n",
      " \n",
      "PID of SUMMED_DICE using the i_min function\n",
      "((0,),) 0.0\n",
      "((0,), (1,)) 0.689439418567615\n",
      "((1,),) 0.0\n",
      "((0, 1),) 2.5849625007211547\n"
     ]
    }
   ],
   "source": [
    "from PyGID.discrete.decompositions import partial_entropy_decomposition, partial_information_decomposition, generalized_information_decomposition\n",
    "\n",
    "# You can do PID with two redundancy functions \n",
    "# i_sx and i_min (i_min is from Finn and Lizier NOT Williams and Beer).\n",
    "\n",
    "print(\"PID of SUMMED_DICE using the i_sx function\")\n",
    "ptw_sx, avg_sx = partial_information_decomposition(\"isx\", (0,1), (2,), SUMMED_DICE)\n",
    "for key in avg_sx.keys():\n",
    "    print(key, avg_sx[key])\n",
    "\n",
    "print(\" \")\n",
    "print(\"PID of SUMMED_DICE using the i_min function\")\n",
    "ptw_min, avg_min = partial_information_decomposition(\"imin\", (0,1), (2,), SUMMED_DICE)\n",
    "for key in avg_min.keys():\n",
    "    print(key, avg_min[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04b6689-7c66-4a37-b8d4-aa026c2069fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PED of XOR_DIST using the i_sx function\n",
      "((0,),) 0.0\n",
      "((0,), (1, 2)) 0.16992500144231237\n",
      "((1,),) 0.0\n",
      "((1,), (0, 2)) 0.16992500144231237\n",
      "((2,),) 0.0\n",
      "((2,), (0, 1)) 0.16992500144231237\n",
      "((0, 1),) 0.0\n",
      "((0, 1), (0, 2)) 0.0\n",
      "((0, 1), (1, 2)) 0.0\n",
      "((0, 2),) 0.0\n",
      "((0, 2), (1, 2)) 0.0\n",
      "((1, 2),) 0.0\n",
      "((0, 1, 2),) 0.0\n",
      "((0,), (1,)) 0.4150374992788438\n",
      "((0,), (1,), (2,)) 0.0\n",
      "((0,), (2,)) 0.4150374992788438\n",
      "((1,), (2,)) 0.4150374992788438\n",
      "((0, 1), (0, 2), (1, 2)) 0.24511249783653155\n",
      " \n",
      "PED of XOR_DIST using the i_min function\n",
      "((0,),) 0.0\n",
      "((0,), (1, 2)) 0.0\n",
      "((1,),) 0.0\n",
      "((1,), (0, 2)) 0.0\n",
      "((2,),) 0.0\n",
      "((2,), (0, 1)) 0.0\n",
      "((0, 1),) 0.0\n",
      "((0, 1), (0, 2)) 0.0\n",
      "((0, 1), (1, 2)) 0.0\n",
      "((0, 2),) 0.0\n",
      "((0, 2), (1, 2)) 0.0\n",
      "((1, 2),) 0.0\n",
      "((0, 1, 2),) 0.0\n",
      "((0,), (1,)) 0.0\n",
      "((0,), (1,), (2,)) 1.0\n",
      "((0,), (2,)) 0.0\n",
      "((1,), (2,)) 0.0\n",
      "((0, 1), (0, 2), (1, 2)) 1.0\n"
     ]
    }
   ],
   "source": [
    "# You can do the same for the PED, using the hsx and hmin functions\n",
    "print(\"PED of XOR_DIST using the i_sx function\")\n",
    "ptw_sx, avg_sx = partial_entropy_decomposition(\"hsx\", XOR_DIST)\n",
    "for key in avg_sx.keys():\n",
    "    print(key, avg_sx[key])\n",
    "\n",
    "print(\" \")\n",
    "print(\"PED of XOR_DIST using the i_min function\")\n",
    "ptw_min, avg_min = partial_entropy_decomposition(\"hmin\", XOR_DIST)\n",
    "for key in avg_min.keys():\n",
    "    print(key, avg_min[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c947720-8429-4f23-8a71-417cd0968679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GID of XOR_DIST || maxent using the i_sx function\n",
      "((0,),) 0.3219280948873624\n",
      "((0,), (1, 2)) -0.12928301694496647\n",
      "((1,),) 0.3219280948873624\n",
      "((1,), (0, 2)) -0.12928301694496647\n",
      "((2,),) 0.3219280948873623\n",
      "((2,), (0, 1)) -0.12928301694496647\n",
      "((0, 1),) 0.16992500144231215\n",
      "((0, 1), (0, 2)) 0.09310940439148174\n",
      "((0, 1), (1, 2)) 0.09310940439148174\n",
      "((0, 2),) 0.16992500144231215\n",
      "((0, 2), (1, 2)) 0.09310940439148174\n",
      "((1, 2),) 0.1699250014423126\n",
      "((0, 1, 2),) 0.24511249783653133\n",
      "((0,), (1,)) -0.19264507794239588\n",
      "((0,), (1,), (2,)) 0.19264507794239588\n",
      "((0,), (2,)) -0.19264507794239588\n",
      "((1,), (2,)) -0.19264507794239588\n",
      "((0, 1), (0, 2), (1, 2)) -0.22686079328030895\n",
      " \n",
      "GID of XOR_DIST || maxent using the i_min function\n",
      "((0,),) 0.0\n",
      "((0,), (1, 2)) 0.0\n",
      "((1,),) 0.0\n",
      "((1,), (0, 2)) 0.0\n",
      "((2,),) 0.0\n",
      "((2,), (0, 1)) 0.0\n",
      "((0, 1),) 0.0\n",
      "((0, 1), (0, 2)) 0.0\n",
      "((0, 1), (1, 2)) 0.0\n",
      "((0, 2),) 0.0\n",
      "((0, 2), (1, 2)) 0.0\n",
      "((1, 2),) 0.0\n",
      "((0, 1, 2),) 1.0\n",
      "((0,), (1,)) 0.0\n",
      "((0,), (1,), (2,)) 0.0\n",
      "((0,), (2,)) 0.0\n",
      "((1,), (2,)) 0.0\n",
      "((0, 1), (0, 2), (1, 2)) 0.0\n"
     ]
    }
   ],
   "source": [
    "# And finally, the GID, which requires specifcying a prior and a posterior. \n",
    "# Maxent is a good prior \n",
    "\n",
    "maxent = maximum_entropy_distribution(XOR_DIST)\n",
    "\n",
    "# You can do the same for the PED, using the hsx and hmin functions\n",
    "print(\"GID of XOR_DIST || maxent using the i_sx function\")\n",
    "ptw_sx, avg_sx = generalized_information_decomposition(\"hsx\", XOR_DIST, maxent)\n",
    "for key in avg_sx.keys():\n",
    "    print(key, avg_sx[key])\n",
    "\n",
    "print(\" \")\n",
    "print(\"GID of XOR_DIST || maxent using the i_min function\")\n",
    "ptw_min, avg_min = generalized_information_decomposition(\"hmin\", XOR_DIST, maxent)\n",
    "for key in avg_min.keys():\n",
    "    print(key, avg_min[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02284a54-2055-4b3e-8a69-68c0ebe24360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity(XOR || Maxent) using h_min = 3.0 bit\n",
      "Complexity(XOR || Maxent) using h_sx = 2.0524674198941355 bit\n"
     ]
    }
   ],
   "source": [
    "# We can quantify the overal representational complexity\n",
    "from PyGID.discrete.decompositions import representational_complexity\n",
    "\n",
    "print(f\"Complexity(XOR || Maxent) using h_min = {representational_complexity(avg_min)} bit\")\n",
    "print(f\"Complexity(XOR || Maxent) using h_sx = {representational_complexity(avg_sx)} bit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
