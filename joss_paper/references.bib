
@article{timme_synergy_2014,
	title = {Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective},
	volume = {36},
	issn = {1573-6873},
	shorttitle = {Synergy, redundancy, and multivariate information measures},
	url = {https://doi.org/10.1007/s10827-013-0458-4},
	doi = {10.1007/s10827-013-0458-4},
	abstract = {Information theory has long been used to quantify interactions between two variables. With the rise of complex systems research, multivariate information measures have been increasingly used to investigate interactions between groups of three or more variables, often with an emphasis on so called synergistic and redundant interactions. While bivariate information measures are commonly agreed upon, the multivariate information measures in use today have been developed by many different groups, and differ in subtle, yet significant ways. Here, we will review these multivariate information measures with special emphasis paid to their relationship to synergy and redundancy, as well as examine the differences between these measures by applying them to several simple model systems. In addition to these systems, we will illustrate the usefulness of the information measures by analyzing neural spiking data from a dissociated culture through early stages of its development. Our aim is that this work will aid other researchers as they seek the best multivariate information measure for their specific research goals and system. Finally, we have made software available online which allows the user to calculate all of the information measures discussedwithin this paper.},
	language = {en},
	number = {2},
	urldate = {2020-02-17},
	journal = {Journal of Computational Neuroscience},
	author = {Timme, Nicholas and Alford, Wesley and Flecker, Benjamin and Beggs, John M.},
	month = apr,
	year = {2014},
	note = {Number: 2},
	pages = {119--140},
	file = {Springer Full Text PDF:/home/thosvarley/Zotero/storage/FTSEBKJR/Timme et al. - 2014 - Synergy, redundancy, and multivariate information .pdf:application/pdf},
}

@article{williams_nonnegative_2010,
	title = {Nonnegative {Decomposition} of {Multivariate} {Information}},
	url = {http://arxiv.org/abs/1004.2515},
	abstract = {Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from first principles the general structure of the information that a set of sources provides about a given variable. We begin with a new definition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy.},
	urldate = {2020-02-25},
	journal = {arXiv:1004.2515 [math-ph, physics:physics, q-bio]},
	author = {Williams, Paul L. and Beer, Randall D.},
	month = apr,
	year = {2010},
	note = {arXiv: 1004.2515},
	keywords = {Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Quantitative Methods, Quantitative Biology - Neurons and Cognition, Physics - Biological Physics, Computer Science - Information Theory, 94A15, Mathematical Physics},
	annote = {Comment: 14 pages, 9 figures},
	file = {arXiv Fulltext PDF:/home/thosvarley/Zotero/storage/PW9METIF/Williams and Beer - 2010 - Nonnegative Decomposition of Multivariate Informat.pdf:application/pdf;arXiv.org Snapshot:/home/thosvarley/Zotero/storage/EBX8BQJF/1004.html:text/html},
}

@article{rosas_quantifying_2019,
	title = {Quantifying {High}-order {Interdependencies} via {Multivariate} {Extensions} of the {Mutual} {Information}},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {http://arxiv.org/abs/1902.11239},
	doi = {10.1103/PhysRevE.100.032305},
	abstract = {This article introduces a model-agnostic approach to study statistical synergy, a form of emergence in which patterns at large scales are not traceable from lower scales. Our framework leverages various multivariate extensions of Shannon's mutual information, and introduces the O-information as a metric capable of characterising synergy- and redundancy-dominated systems. We develop key analytical properties of the O-information, and study how it relates to other metrics of high-order interactions from the statistical mechanics and neuroscience literature. Finally, as a proof of concept, we use the proposed framework to explore the relevance of statistical synergy in Baroque music scores.},
	number = {3},
	urldate = {2020-03-29},
	journal = {Physical Review E},
	author = {Rosas, Fernando and Mediano, Pedro A. M. and Gastpar, Michael and Jensen, Henrik J.},
	month = sep,
	year = {2019},
	note = {Number: 3
arXiv: 1902.11239},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Information Theory},
	pages = {032305},
	file = {arXiv Fulltext PDF:/home/thosvarley/Zotero/storage/PBSW5N4W/Rosas et al. - 2019 - Quantifying High-order Interdependencies via Multi.pdf:application/pdf;arXiv.org Snapshot:/home/thosvarley/Zotero/storage/UWC3GLCY/1902.html:text/html},
}

@book{cover_elements_2012,
	title = {Elements of {Information} {Theory}},
	isbn = {978-1-118-58577-1},
	abstract = {The latest edition of this classic is updated with new problem sets and material   The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	month = nov,
	year = {2012},
	note = {Google-Books-ID: VWq5GG6ycxMC},
	keywords = {Computers / General, Computers / Information Technology},
}

@article{ince_partial_2017,
	title = {The {Partial} {Entropy} {Decomposition}: {Decomposing} multivariate entropy and mutual information via pointwise common surprisal},
	shorttitle = {The {Partial} {Entropy} {Decomposition}},
	url = {http://arxiv.org/abs/1702.01591},
	abstract = {Obtaining meaningful quantitative descriptions of the statistical dependence within multivariate systems is a difficult open problem. Recently, the Partial Information Decomposition (PID) was proposed to decompose mutual information (MI) about a target variable into components which are redundant, unique and synergistic within different subsets of predictor variables. Here, we propose to apply the elegant formalism of the PID to multivariate entropy, resulting in a Partial Entropy Decomposition (PED). We implement the PED with an entropy redundancy measure based on pointwise common surprisal; a natural definition which is closely related to the definition of MI. We show how this approach can reveal the dyadic vs triadic generative structure of multivariate systems that are indistinguishable with classical Shannon measures. The entropy perspective also shows that misinformation is synergistic entropy and hence that MI itself includes both redundant and synergistic effects. We show the relationships between the PED and MI in two predictors, and derive two alternative information decompositions which we illustrate on several example systems. This reveals that in entropy terms, univariate predictor MI is not a proper subset of the joint MI, and we suggest this previously unrecognised fact explains in part why obtaining a consistent PID has proven difficult. The PED also allows separate quantification of mechanistic redundancy (related to the function of the system) versus source redundancy (arising from dependencies between inputs); an important distinction which no existing methods can address. The new perspective provided by the PED helps to clarify some of the difficulties encountered with the PID approach and the resulting decompositions provide useful tools for practical data analysis across a wide range of application areas.},
	language = {en},
	urldate = {2020-08-24},
	journal = {arXiv:1702.01591 [cs, math, q-bio, stat]},
	author = {Ince, Robin A. A.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.01591},
	keywords = {Quantitative Biology - Quantitative Methods, Statistics - Methodology, Quantitative Biology - Neurons and Cognition, Mathematics - Statistics Theory, Computer Science - Information Theory},
	annote = {Comment: Added Section 3.7 (Quantifying source vs mechanistic redundancy) and Section 3.8 (Shared entropy as a measure of dependence: pure mutual information) and updated abstract, results, and discussion accordingly},
	file = {Ince - 2017 - The Partial Entropy Decomposition Decomposing mul.pdf:/home/thosvarley/Zotero/storage/JVLC3UWW/Ince - 2017 - The Partial Entropy Decomposition Decomposing mul.pdf:application/pdf},
}

@article{lizier_jidt_2014,
	title = {{JIDT}: {An} {Information}-{Theoretic} {Toolkit} for {Studying} the {Dynamics} of {Complex} {Systems}},
	volume = {1},
	issn = {2296-9144},
	shorttitle = {{JIDT}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2014.00011/full},
	doi = {10.3389/frobt.2014.00011},
	abstract = {Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics and Artificial Life. This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyse the dynamics of complex systems in these fields. We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data. While the toolkit provides classic information-theoretic measures (e.g. entropy, mutual information, conditional mutual information), it ultimately focusses on implementing higher-level measures for information dynamics. That is, JIDT focusses on quantifying information storage, transfer and modification, and the dynamics of these operations in space and time. For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants. JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g. Gaussian, box-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped at run-time due to Java's object-oriented polymorphism. Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave, Python and other environments. We present the principles behind the code design, and provide several examples to guide users.},
	language = {English},
	urldate = {2020-10-23},
	journal = {Frontiers in Robotics and AI},
	author = {Lizier, Joseph T.},
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Information Theory, MATLAB, transfer entropy, complex networks, python, complex systems, information storage, information transfer, Java, Octave},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/7BTRHG4V/Lizier - 2014 - JIDT An Information-Theoretic Toolkit for Studyin.pdf:application/pdf},
}

@article{wollstadt_idtxl_2019,
	title = {{IDTxl}: {The} {Information} {Dynamics} {Toolkit} xl: a {Python} package for the efficient analysis of multivariate information dynamics in networks},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {{IDTxl}},
	url = {https://joss.theoj.org/papers/10.21105/joss.01081},
	doi = {10.21105/joss.01081},
	abstract = {Wollstadt et al., (2019). IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks. Journal of Open Source Software, 4(34), 1081, https://doi.org/10.21105/joss.01081},
	language = {en},
	number = {34},
	urldate = {2020-11-21},
	journal = {Journal of Open Source Software},
	author = {Wollstadt, Patricia and Lizier, Joseph T. and Vicente, Raul and Finn, Conor and Martinez-Zarzuela, Mario and Mediano, Pedro and Novelli, Leonardo and Wibral, Michael},
	month = feb,
	year = {2019},
	note = {Number: 34},
	pages = {1081},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/VEXJ56MH/Wollstadt et al. - 2019 - IDTxl The Information Dynamics Toolkit xl a Pyth.pdf:application/pdf;Snapshot:/home/thosvarley/Zotero/storage/F8V9992C/joss.html:text/html},
}

@article{james_dit_2018,
	title = {dit: a {Python} package for discrete information theory},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {dit},
	url = {http://joss.theoj.org/papers/10.21105/joss.00738},
	doi = {10.21105/joss.00738},
	language = {en},
	number = {25},
	urldate = {2020-12-17},
	journal = {Journal of Open Source Software},
	author = {James, Ryan and Ellison, Christopher and Crutchfield, James},
	month = may,
	year = {2018},
	note = {Number: 25},
	pages = {738},
	file = {G. James et al. - 2018 - dit a Python package for discrete information the.pdf:/home/thosvarley/Zotero/storage/P4CN7UR8/G. James et al. - 2018 - dit a Python package for discrete information the.pdf:application/pdf},
}

@article{james_multivariate_2017,
	title = {Multivariate {Dependence} beyond {Shannon} {Information}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/19/10/531},
	doi = {10.3390/e19100531},
	abstract = {Accurately determining dependency structure is critical to understanding a complex system’s organization. We recently showed that the transfer entropy fails in a key aspect of this—measuring information flow—due to its conflation of dyadic and polyadic relationships. We extend this observation to demonstrate that Shannon information measures (entropy and mutual information, in their conditional and multivariate forms) can fail to accurately ascertain multivariate dependencies due to their conflation of qualitatively different relations among variables. This has broad implications, particularly when employing information to express the organization and mechanisms embedded in complex systems, including the burgeoning efforts to combine complex network theory with information theory. Here, we do not suggest that any aspect of information theory is wrong. Rather, the vast majority of its informational measures are simply inadequate for determining the meaningful relationships among variables within joint probability distributions. We close by demonstrating that such distributions exist across an arbitrary set of variables.},
	language = {en},
	number = {10},
	urldate = {2022-02-01},
	journal = {Entropy},
	author = {James, Ryan G. and Crutchfield, James P.},
	month = oct,
	year = {2017},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {transfer entropy, network science, partial information decomposition, causation entropy, stochastic process},
	pages = {531},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/4H2N7QGV/James and Crutchfield - 2017 - Multivariate Dependence beyond Shannon Information.pdf:application/pdf;Snapshot:/home/thosvarley/Zotero/storage/9RBTPQUT/htm.html:text/html},
}

@article{goodwell_debatesdoes_2020,
	title = {Debates—{Does} {Information} {Theory} {Provide} a {New} {Paradigm} for {Earth} {Science}? {Causality}, {Interaction}, and {Feedback}},
	volume = {56},
	issn = {1944-7973},
	shorttitle = {Debates—{Does} {Information} {Theory} {Provide} a {New} {Paradigm} for {Earth} {Science}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019WR024940},
	doi = {10.1029/2019WR024940},
	abstract = {The concept of causal interactions between components is an integral part of hydrology and Earth system sciences. Modelers, decision makers, scientists, and other water resources stakeholders all utilize some notion of cause-and-effect to understand processes, make decisions, and infer how systems react to change. However, there are different perspectives on the meaning of causality in complex systems and, further, different frameworks and methodologies with which to detect causal interactions. We propose here that information theory (IT) provides a compelling framework for the detection of causality and discuss approaches for several levels of analyses that capture interactions that range from pairwise to multivariate in nature. We illustrate these types of analyses with an example based on weather station time series variables, in which variables may interact pairwise or jointly and on short to long time scales. In general, many unsolved or even unanticipated questions in the hydrologic sciences could benefit from this perspective.},
	language = {en},
	number = {2},
	urldate = {2022-06-14},
	journal = {Water Resources Research},
	author = {Goodwell, Allison E. and Jiang, Peishi and Ruddell, Benjamin L. and Kumar, Praveen},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019WR024940},
	pages = {e2019WR024940},
	annote = {e2019WR024940 10.1029/2019WR024940},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/U4A9PYAE/Goodwell et al. - 2020 - Debates—Does Information Theory Provide a New Para.pdf:application/pdf;Snapshot:/home/thosvarley/Zotero/storage/BEZUX9R8/2019WR024940.html:text/html},
}

@article{goodwell_its_2020,
	title = {“{It}’s {Raining} {Bits}”: {Patterns} in {Directional} {Precipitation} {Persistence} across the {United} {States}},
	volume = {21},
	issn = {1525-7541, 1525-755X},
	shorttitle = {“{It}’s {Raining} {Bits}”},
	url = {https://journals.ametsoc.org/view/journals/hydr/21/12/jhm-d-20-0134.1.xml},
	doi = {10.1175/JHM-D-20-0134.1},
	abstract = {Abstract The spatial and temporal ordering of precipitation occurrence impacts ecosystems, streamflow, and water availability. For example, both large-scale climate patterns and local landscapes drive weather events, and the typical speeds and directions of these events moving across a basin dictate the timing of flows at its outlet. We address the predictability of precipitation occurrence at a given location, based on the knowledge of past precipitation at surrounding locations. We identify “dominant directions of precipitation influence” across the continental United States based on a gridded daily dataset. Specifically, we apply information theory–based measures that characterize dominant directions and strengths of spatial and temporal precipitation dependencies. On a national average, this dominant direction agrees with the prevalent direction of weather movement from west to east across the country, but regional differences reflect topographic divides, precipitation gradients, and different climatic drivers of precipitation. Trends in these information relationships and their correlations with climate indices over the past 70 years also show seasonal and spatial divides. This study expands upon a framework of information-based predictability to answer questions about spatial connectivity in addition to temporal persistence. The methods presented here are generally useful to understand many aspects of weather and climate variability.},
	language = {EN},
	number = {12},
	urldate = {2022-06-14},
	journal = {Journal of Hydrometeorology},
	author = {Goodwell, Allison E.},
	month = dec,
	year = {2020},
	note = {Publisher: American Meteorological Society
Section: Journal of Hydrometeorology},
	pages = {2907--2921},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/L6S2ANAD/Goodwell - 2020 - “It’s Raining Bits” Patterns in Directional Preci.pdf:application/pdf;Snapshot:/home/thosvarley/Zotero/storage/3LU7BD5E/jhm-d-20-0134.1.html:text/html},
}

@inproceedings{hagberg_exploring_2008,
	address = {Pasadena, CA USA},
	title = {Exploring {Network} {Structure}, {Dynamics}, and {Function} using {NetworkX}},
	booktitle = {Proceedings of the 7th {Python} in {Science} {Conference}},
	author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J.},
	editor = {Varoquaux, Gaël and Vaught, Travis and Millman, Jarrod},
	year = {2008},
	pages = {11 -- 15},
}

@article{varley_generalized_2024,
	title = {Generalized decomposition of multivariate information},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0297128},
	doi = {10.1371/journal.pone.0297128},
	abstract = {Since its introduction, the partial information decomposition (PID) has emerged as a powerful, information-theoretic technique useful for studying the structure of (potentially higher-order) interactions in complex systems. Despite its utility, the applicability of the PID is restricted by the need to assign elements as either “sources” or “targets”, as well as the specific structure of the mutual information itself. Here, I introduce a generalized information decomposition that relaxes the source/target distinction while still satisfying the basic intuitions about information. This approach is based on the decomposition of the Kullback-Leibler divergence, and consequently allows for the analysis of any information gained when updating from an arbitrary prior to an arbitrary posterior. As a result, any information-theoretic measure that can be written as a linear combination of Kullback-Leibler divergences admits a decomposition in the style of Williams and Beer, including the total correlation, the negentropy, and the mutual information as special cases. This paper explores how the generalized information decomposition can reveal novel insights into existing measures, as well as the nature of higher-order synergies. We show that synergistic information is intimately related to the well-known Tononi-Sporns-Edelman (TSE) complexity, and that synergistic information requires a similar integration/segregation balance as a high TSE complexity. Finally, I end with a discussion of how this approach fits into other attempts to generalize the PID and the possibilities for empirical applications.},
	language = {en},
	number = {2},
	urldate = {2024-02-06},
	journal = {PLOS ONE},
	author = {Varley, Thomas F.},
	month = feb,
	year = {2024},
	note = {Publisher: Public Library of Science},
	keywords = {Entropy, Probability distribution, Complex systems, Information theory, Information entropy, Sensory perception, Chemical elements, Multivariate random variables},
	pages = {e0297128},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/GRPLC2D2/Varley - 2024 - Generalized decomposition of multivariate informat.pdf:application/pdf},
}

@article{varley_serotonergic_2024,
	title = {The serotonergic psychedelic {N},{N}-dipropyltryptamine alters information-processing dynamics in in vitro cortical neural circuits},
	issn = {2472-1751},
	url = {https://direct.mit.edu/netn/article/doi/10.1162/netn_a_00408/123886/The-serotonergic-psychedelic-N-N},
	doi = {10.1162/netn_a_00408},
	abstract = {Abstract
            Most of the recent work in psychedelic neuroscience has been done using non-invasive neuroimaging, with data recorded from the brains of adult volunteers under the influence of a variety of drugs. While this data provides holistic insights into the effects of psychedelics on whole-brain dynamics, the effects of psychedelics on the meso-scale dynamics of neuronal circuits remains much less explored. Here, we report the effects of the serotonergic psychedelic N,N-diproptyltryptamine (DPT) on information-processing dynamics in a sample of in vitro organotypic cultures of cortical tissue from post-natal rats. Three hours of spontaneous activity were recorded: an hour of pre-drug control, and hour of exposure to 10 μM DPT solution, and a final hour of washout, once again under control conditions. We found that DPT reversibly alters information dynamics in multiple ways: first, the DPT condition was associated with higher entropy of spontaneous firing activity and reduced the amount of time information was stored in individual neurons. Second, DPT also reduced the reversibility of neural activity, increasing the entropy produced and suggesting a drive away from equilibrium. Third, DPT altered the structure of neuronal circuits, decreasing the overall information flow coming into each neuron, but increasing the number of weak connections, creating a dynamic that combines elements of integration and disintegration. Finally, DPT decreased the higher-order statistical synergy present in sets of three neurons. Collectively, these results paint a complex picture of how psychedelics regulate information processing in meso-scale neuronal networks in cortical tissue. Implications for existing hypotheses of psychedelic action, such as the Entropic Brain Hypothesis, are discussed.},
	language = {en},
	urldate = {2024-08-13},
	journal = {Network Neuroscience},
	author = {Varley, Thomas F. and Havert, Daniel and Fosque, Leandro and Alipour, Abolfazl and Weerawongphrom, Naruepon and Naganobori, Hiroki and O’Shea, Lily and Pope, Maria and Beggs, John},
	month = aug,
	year = {2024},
	pages = {1--35},
	file = {Varley et al. - 2024 - The serotonergic psychedelic N,N-dipropyltryptamin.pdf:/home/thosvarley/Zotero/storage/U4C9MQSS/Varley et al. - 2024 - The serotonergic psychedelic N,N-dipropyltryptamin.pdf:application/pdf},
}

@misc{varley_information_2024,
	title = {Information {Theory} for {Complex} {Systems} {Scientists}},
	url = {http://arxiv.org/abs/2304.12482},
	doi = {10.48550/arXiv.2304.12482},
	abstract = {In the 21st century, many of the crucial scientific and technical issues facing humanity can be understood as problems associated with understanding, modelling, and ultimately controlling complex systems: systems comprised of a large number of non-trivially interacting components whose collective behaviour can be difficult to predict. Information theory, a branch of mathematics historically associated with questions about encoding and decoding messages, has emerged as something of a lingua franca for those studying complex systems, far exceeding its original narrow domain of communication systems engineering. In the context of complexity science, information theory provides a set of tools which allow researchers to uncover the statistical and effective dependencies between interacting components; relationships between systems and their environment; mereological whole-part relationships; and is sensitive to non-linearities missed by commonly parametric statistical models. In this review, we aim to provide an accessible introduction to the core of modern information theory, aimed specifically at aspiring (and established) complex systems scientists. This includes standard measures, such as Shannon entropy, relative entropy, and mutual information, before building to more advanced topics, including: information dynamics, measures of statistical complexity, information decomposition, and effective network inference. In addition to detailing the formal definitions, in this review we make an effort to discuss how information theory can be interpreted and develop the intuition behind abstract concepts like "entropy," in the hope that this will enable interested readers to understand what information is, and how it is used, at a more fundamental level.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Varley, Thomas F.},
	month = aug,
	year = {2024},
	note = {arXiv:2304.12482 [physics, q-bio, stat]},
	keywords = {Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Quantitative Methods, Computer Science - Information Theory, Statistics - Other Statistics},
	file = {arXiv Fulltext PDF:/home/thosvarley/Zotero/storage/DZQNMYVL/Varley - 2024 - Information Theory for Complex Systems Scientists.pdf:application/pdf;arXiv.org Snapshot:/home/thosvarley/Zotero/storage/7MGB3WK8/2304.html:text/html},
}

@article{blackiston_revealing_2025,
	title = {Revealing non-trivial information structures in aneural biological tissues via functional connectivity},
	volume = {21},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012149},
	doi = {10.1371/journal.pcbi.1012149},
	abstract = {A central challenge in the progression of a variety of open questions in biology, such as morphogenesis, wound healing, and development, is learning from empirical data how information is integrated to support tissue-level function and behavior. Information-theoretic approaches provide a quantitative framework for extracting patterns from data, but so far have been predominantly applied to neuronal systems at the tissue-level. Here, we demonstrate how time series of Ca2+ dynamics can be used to identify the structure and information dynamics of other biological tissues. To this end, we expressed the calcium reporter GCaMP6s in an organoid system of explanted amphibian epidermis derived from the African clawed frog Xenopus laevis, and imaged calcium activity pre- and post- a puncture injury, for six replicate organoids. We constructed functional connectivity networks by computing mutual information between cells from time series derived using medical imaging techniques to track intracellular Ca2+. We analyzed network properties including degree distribution, spatial embedding, and modular structure. We find organoid networks exhibit potential evidence for more connectivity than null models, with our models displaying high degree hubs and mesoscale community structure with spatial clustering. Utilizing functional connectivity networks, our model suggests the tissue retains non-random features after injury, displays long range correlations and structure, and non-trivial clustering that is not necessarily spatially dependent. In the context of this reconstruction method our results suggest increased integration after injury, possible cellular coordination in response to injury, and some type of generative structure of the anatomy. While we study Ca2+ in Xenopus epidermal cells, our computational approach and analyses highlight how methods developed to analyze functional connectivity in neuronal tissues can be generalized to any tissue and fluorescent signal type. We discuss expanded methods of analyses to improve models of non-neuronal information processing highlighting the potential of our framework to provide a bridge between neuroscience and more basal modes of information processing.},
	language = {en},
	number = {4},
	urldate = {2025-04-15},
	journal = {PLOS Computational Biology},
	author = {Blackiston, Douglas and Dromiack, Hannah and Grasso, Caitlin and Varley, Thomas F. and Moore, Douglas G. and Srinivasan, Krishna Kannan and Sporns, Olaf and Bongard, Joshua and Levin, Michael and Walker, Sara I.},
	month = apr,
	year = {2025},
	note = {Publisher: Public Library of Science},
	keywords = {Information processing, Network analysis, Calcium imaging, Preprocessing, Wound healing, Built structures, Calcium signaling, Organoids},
	pages = {e1012149},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/SJD38HVI/Blackiston et al. - 2025 - Revealing non-trivial information structures in aneural biological tissues via functional connectivi.pdf:application/pdf},
}

@misc{sparacino_decomposing_2025,
	title = {Decomposing {Multivariate} {Information} {Rates} in {Networks} of {Random} {Processes}},
	url = {http://arxiv.org/abs/2502.04555},
	doi = {10.48550/arXiv.2502.04555},
	abstract = {The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scalespecific higher-order interactions that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology.},
	language = {en},
	urldate = {2025-04-30},
	publisher = {arXiv},
	author = {Sparacino, Laura and Mijatovic, Gorana and Antonacci, Yuri and Ricci, Leonardo and Marinazzo, Daniele and Stramaglia, Sebastiano and Faes, Luca},
	month = feb,
	year = {2025},
	note = {arXiv:2502.04555 [stat]},
	keywords = {Statistics - Methodology, Computer Science - Information Theory, Mathematics - Information Theory},
	file = {PDF:/home/thosvarley/Zotero/storage/F58464CT/Sparacino et al. - 2025 - Decomposing Multivariate Information Rates in Networks of Random Processes.pdf:application/pdf},
}

@article{rajpal_synergistic_2025,
	title = {Synergistic small worlds that drive technological sophistication},
	volume = {4},
	issn = {2752-6542},
	url = {https://doi.org/10.1093/pnasnexus/pgaf102},
	doi = {10.1093/pnasnexus/pgaf102},
	abstract = {It is a well-known fact that economic growth goes hand in hand with improvements in technological sophistication. While critical to such sophistication, the nature and underlying structure of the input interactions taking place inside production processes remain opaque, at least in the study of large systems such as industries and entire economies. We develop a method to quantify the degree of input complementarity in production processes form input–output data. We propose that the information-theoretic concept of synergistic information is analog to economic complementarity and exploit this link to create a data-driven approach that does not require the ex ante assumption of production functions. In contrast to alternative empirical approaches, our method is able identify input–input interactions and to quantify their contribution to output, revealing an input–input synergistic interaction network that characterizes an industry’s productive technology. We find that more sophisticated industries tend to exhibit highly modular small-world topologies; with the tertiary sector as its central connective core. Overall, countries and industries that have a well-established connective core and specialized modules exhibit higher economic complexity, higher output, and lower emissions. The proposed method provides a framework to identify key relationships in the economy that can enhance economic performance.},
	number = {4},
	urldate = {2025-05-02},
	journal = {PNAS Nexus},
	author = {Rajpal, Hardik and Guerrero, Omar},
	month = apr,
	year = {2025},
	pages = {pgaf102},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/I7JGL2XE/Rajpal and Guerrero - 2025 - Synergistic small worlds that drive technological sophistication.pdf:application/pdf;Snapshot:/home/thosvarley/Zotero/storage/DM2ULKUR/8096462.html:text/html},
}

@article{kim_predicting_2020,
	title = {Predicting the {Direction} of {US} {Stock} {Prices} {Using} {Effective} {Transfer} {Entropy} and {Machine} {Learning} {Techniques}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9119388},
	doi = {10.1109/ACCESS.2020.3002174},
	abstract = {This study aims to predict the direction of US stock prices by integrating time-varying effective transfer entropy (ETE) and various machine learning algorithms. At first, we explore that the ETE based on 3 and 6 months moving windows can be regarded as the market explanatory variable by analyzing the association between the financial crises and Granger-causal relationships among the stocks. Then, we discover that the prediction performance on the stock price direction can be improved when the ETE driven variable is integrated as a new feature in the logistic regression, multilayer perceptron, random forest, XGBoost, and long short-term memory network. Meanwhile, we suggest utilizing the adjusted accuracy derived from the risk-adjusted return in finance as a prediction performance measure. Lastly, we confirm that the multilayer perceptron and long short-term memory network are more suitable for stock price prediction. This study is the first attempt to predict the stock price direction using ETE, which can be conveniently applied to the practical field.},
	urldate = {2025-05-02},
	journal = {IEEE Access},
	author = {Kim, Sondo and Ku, Seungmo and Chang, Woojin and Song, Jae Wook},
	year = {2020},
	keywords = {Entropy, Machine learning, information entropy, Econophysics, Stock markets, machine learning, Correlation, time series analysis, effective transfer entropy, Exchange rates, feature engineering, Indexes, Machine learning algorithms, prediction algorithms, stock markets},
	pages = {111660--111682},
	file = {Full Text PDF:/home/thosvarley/Zotero/storage/H745YK4W/Kim et al. - 2020 - Predicting the Direction of US Stock Prices Using Effective Transfer Entropy and Machine Learning Te.pdf:application/pdf},
}

@inproceedings{blanc_quantifying_2008,
	address = {Marseille, France.},
	title = {Quantifying {Neural} {Correlations} {Using} {Lempel}-{Ziv} {Complexity}},
	abstract = {Spike train analysis generally focuses on two aims: (1) the estimate of the neuronal information quantity, and (2) the quantiﬁcation of spikes or bursts synchronization. We introduce here a new multivariate index based on LempelZiv complexity for spike train analysis. This index, called mutual Lempel-Ziv complexity (MLZC), can both measure spikes correlations and estimate the information carried in spike trains (i.e. characterize the dynamic process). Using simulated spike trains from a Poisson process, we show that the MLZC is able to quantify spike correlations. In addition, using bursting activity generated by electrically coupled Hindmarsh-Rose neurons, the MLZC is able to quantify and characterize bursts synchronization, when classical measures fail.},
	language = {en},
	booktitle = {Neurocomp08},
	author = {Blanc, Jean-Luc and Schmidt, Nicolas and Bonnier, Loic and Pezard, Laurent and Lesne, Annick},
	month = oct,
	year = {2008},
	file = {PDF:/home/thosvarley/Zotero/storage/YW2NVV5Q/Blanc et al. - Quantifying Neural Correlations Using Lempel-Ziv Complexity.pdf:application/pdf},
}
