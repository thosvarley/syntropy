<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>syntropy.gaussian package &#8212; Syntropy 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="syntropy.knn package" href="syntropy.knn.html" />
    <link rel="prev" title="syntropy.discrete package" href="syntropy.discrete.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-syntropy.gaussian">
<span id="syntropy-gaussian-package"></span><h1>syntropy.gaussian package<a class="headerlink" href="#module-syntropy.gaussian" title="Link to this heading">¶</a></h1>
<p>Created on Mon Feb 10 21:09:55 2025</p>
<p>&#64;author: thosvarley</p>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.differential_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">differential_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#differential_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.differential_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the expected differential entropy of a multivariate
distribution parameterized by a covariance matrix using a
Gaussian estimator.</p>
<p>The differential entropy is given by:</p>
<div class="math notranslate nohighlight">
\[H(X) = \int dx P(x)\log P(x)\]</div>
<p>And if <span class="math notranslate nohighlight">\(X\)</span> is drawn from a k-dimensional Gaussian, it is equal to</p>
<div class="math notranslate nohighlight">
\[H(X) = \frac{k}{2}\log 2\pi\textnormal{e} + \frac{1}{2}\log|\Sigma|\]</div>
<p>Where <span class="math notranslate nohighlight">\(|\Sigma|\)</span> is the determinant of the covariance matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_differential_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_differential_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#local_differential_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_differential_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the framewise differential entropy for a set of variables.</p>
<div class="math notranslate nohighlight">
\[h(x) = -\log P(x)\]</div>
<p>For data drawn from a k-dimensional Gaussian</p>
<div class="math notranslate nohighlight">
\[P(x) = (2\pi)^{-k/2}|\Sigma|^{-1/2}\textnormal{e}^{\frac{-(x - \mu)^\mathrm{T} \Sigma^{-1}(x - \mu)}{2}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If none is provided, it is computed from the data object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The series of pointwise entropies.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.conditional_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">conditional_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#conditional_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.conditional_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the conditional entropy of X given Y using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[H(X|Y) = H(X,Y) - H(Y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the variables to compute the conditional entropy on.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the conditioning set.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If none is provided, it is computed from the data object.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_conditional_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_conditional_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#local_conditional_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_conditional_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local condition entropy for every sample in data using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[h(x|y) = h(x,y) - h(y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the variables to compute the conditional entropy on.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the conditioning set.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If none is provided, it is computed from the data object.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the mutual information between two (potentially multivariate) sets of elements.</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \\
       &amp;= H(X) - H(X|Y) \\
       &amp;= H(Y) - H(Y|X) \\
       &amp;= H(X,Y) - H(X|Y) - H(Y|X)\end{split}\]</div>
<p>For Gaussian random variables:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) = \frac{1}{2}\log\frac{|\Sigma_{X}||\Sigma_{Y}|}{|\Sigma_{XY}|}\]</div>
<p>In the particular case where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are univariate, the mutual information can be computed directly from the Pearson correlation coefficient <span class="math notranslate nohighlight">\(r\)</span>:</p>
<div class="math notranslate nohighlight">
\[I(X;Y) = \frac{-\log(1-r^{2})}{2}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the source variables. Can be multivariate.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the idxs_y variable. Can be multivariate.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#local_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local mutual information between X and Y for every sample in data using Gaussian estimation.
Note that the local mutual information can be negative.</p>
<div class="math notranslate nohighlight">
\[\begin{split}i(x;y) &amp;= h(x) + h(y) - h(x,y) \\
       &amp;= \log\frac{p(x|y)}{p(x)} \\
       &amp;= \log\frac{p(y|x)}{p(y)} \\
       &amp;= \log\frac{p(x,y)}{p(x)p(y)} \\\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the source variables. Can be multivariate.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the idxs_y variable. Can be multivariate.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If none is provided, it is computed from the data object.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.conditional_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">conditional_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#conditional_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.conditional_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the expected mutual information between a set of variables X
and Y, conditional on a third set Z.</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X,Y|Z) &amp;= H(X|Z) + H(Y|Z) - H(X,Y|Z) \\
         &amp;= I(X;Y,Z) - I(X;Z)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the X variables. Can be multivariate.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the Y variable. Can be multivariate.</p></li>
<li><p><strong>idxs_z</strong> (<em>tuple</em>) – The indices of the conditioning set. Can be multivariate.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_conditional_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_conditional_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#local_conditional_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_conditional_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local conditional mutual information between
two sets of variables X and Y, conditional on another set Z.</p>
<div class="math notranslate nohighlight">
\[\begin{split}i(x,y|z) &amp;= h(x|z) + h(y|z) - h(x,y|z) \\
         &amp;= i(x;y,z) - i(x;z)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the X variables. Can be multivariate.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the Y variable. Can be multivariate.</p></li>
<li><p><strong>idxs_z</strong> (<em>tuple</em>) – The indices of the conditioning set. Can be multivariate.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If none is provided, it is computed from the data object.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.kullback_leibler_divergence">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">kullback_leibler_divergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov_posterior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_prior</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#kullback_leibler_divergence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.kullback_leibler_divergence" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Gaussian Kullback-Leibler divergence between two <span class="math notranslate nohighlight">\(k\)</span>-dimensional multivariate Gaussians parameterized by covariance matrices.</p>
<div class="math notranslate nohighlight">
\[D_{KL}(\mathcal{N}_0 || \mathcal{N}_1) = \frac{1}{2}[ \operatorname{tr}(\Sigma_{1}^{-1}\Sigma_{0}) - k + (\mu_1 - \mu_0)^\mathsf{T} \Sigma_{1}^{-1}(\mu_1 - \mu_0) + \log\frac{|\Sigma_{1}|}{|\Sigma_{0}|}]\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov_posterior</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the posterior distribution.</p></li>
<li><p><strong>cov_prior</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the prior distribution .</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_kullback_leibler_divergence">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_kullback_leibler_divergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov_posterior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/shannon.html#local_kullback_leibler_divergence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_kullback_leibler_divergence" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local Kullback-Leibler divergence between the
posterior and the prior for every sample in the data.
The local KL divergence is a rarely used measure.</p>
<div class="math notranslate nohighlight">
\[d_{kl}^{P||Q}(x) = h^{Q}(x) - h^{P}(x)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov_posterior</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the posterior distribution.</p></li>
<li><p><strong>cov_prior</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the prior distribution .</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data, assumed to be in channels x samples format.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local Kullback-Leibler divergence.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Varley, T. F. (2024).
Generalized decomposition of multivariate information.
PLOS ONE, 19(2), e0297128.
<a class="reference external" href="https://doi.org/10.1371/journal.pone.0297128">https://doi.org/10.1371/journal.pone.0297128</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#local_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>The local total correlation. Note that this measure can be negative.</p>
<div class="math notranslate nohighlight">
\[tc(x) = \sum_{i=1}^{N}h(x_i) - h(x)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>inptuts</strong> (<em>tuple</em>) – The indices of the channels to include.</p></li>
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local total correaltion for each frame.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Scagliarini, T., Marinazzo, D., Guo, Y., Stramaglia, S., &amp; Rosas, F. E. (2022).
Quantifying high-order interdependencies on individual patterns via the local O-information: Theory and applications to music analysis.
Physical Review Research, 4(1), 013184.
<a class="reference external" href="https://doi.org/10.1103/PhysRevResearch.4.013184">https://doi.org/10.1103/PhysRevResearch.4.013184</a></p>
<p>Pope, M., Varley, T. F., Grazia Puxeddu, M., Faskowitz, J., &amp; Sporns, O. (2025).
Time-varying synergy/redundancy dominance in the human cerebral cortex.
Journal of Physics: Complexity, 6(1), 015015.
<a class="reference external" href="https://doi.org/10.1088/2632-072X/adbaa9">https://doi.org/10.1088/2632-072X/adbaa9</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>The expected total correlation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}TC(X) &amp;= D_{KL}(P(X) || \prod_{i=1}^{N}P(X_i) \\
      &amp;= \sum_{i=1}^{N}H(X_i) - H(X)\end{split}\]</div>
<p>For Gaussian random variables, the estimator is:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{TC}(X) = \frac{-\log R}{2}\]</div>
</div></blockquote>
<p>Where <span class="math notranslate nohighlight">\(R\)</span> is the Pearson correlation matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The expected total correlation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Watanabe, S. (1960). Information Theoretical Analysis of Multivariate Correlation.
IBM Journal of Research and Development, 4(1), Article 1.
<a class="reference external" href="https://doi.org/10.1147/rd.41.0066">https://doi.org/10.1147/rd.41.0066</a></p>
<p>Tononi, G., Sporns, O., &amp; Edelman, G. M. (1994).
A measure for brain complexity: Relating functional segregation and integration in the nervous system.
Proceedings of the National Academy of Sciences, 91(11), Article 11.
<a class="reference external" href="https://doi.org/10.1073/pnas.91.11.5033">https://doi.org/10.1073/pnas.91.11.5033</a></p>
<p>Pascual-Marqui, R. D., Kochi, K., &amp; Kinoshita, T. (2025).
Total/dual correlation/coherence, redundancy/synergy, complexity, and O-information for real and complex valued multivariate data
(No. arXiv:2507.08773). arXiv.
https://doi.org/10.48550/arXiv.2507.08773</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_dual_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_dual_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#local_dual_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_dual_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>Computes the dual total correlation using Gaussian estimation. Note that this measure can be negative.</p>
<div class="math notranslate nohighlight">
\[\begin{split}dtc(x) &amp;= h(x) - \sum_{i=1}^{N}h(x_i|x^{-i}) \\
       &amp;= (N-1)\times tc(x) - \sum_{i=1}^{N}tc(x^{-i})\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If unspecified it is computed directly from the data.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The series of local dual total correlations.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating].</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.dual_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">dual_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#dual_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.dual_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>Computes the dual total correlation using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}DTC(X) &amp;= H(X) - \sum_{i=1}^{N}H(X_i|X^{-i}) \\
       &amp;= (N-1)\times TC(X) - \sum_{i=1}^{N}TC(X^{-i})\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution. .</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The expected dual total correlation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Abdallah, S. A., &amp; Plumbley, M. D. (2012).
A measure of statistical complexity based on predictive information with application to finite spin systems.
Physics Letters A, 376(4), 275–281.
<a class="reference external" href="https://doi.org/10.1016/j.physleta.2011.10.066">https://doi.org/10.1016/j.physleta.2011.10.066</a></p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_s_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_s_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#local_s_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_s_information" title="Link to this definition">¶</a></dt>
<dd><p>Compute local S-information using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\sigma(X) &amp;= \sum_{i=1}^{N}i(x_i;x^{-i}) \\
           &amp;= N\times tc(x) - \sum_{i=1}^{N}tc(x^{-i}) \\
           &amp;= tc(x) + dtc(x)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If unspecified it is computed directly from the data.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The series of local S-information.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating].</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.s_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">s_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#s_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.s_information" title="Link to this definition">¶</a></dt>
<dd><p>Compute S-information using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma(X) &amp;= \sum_{i=1}^{N}I(X_i;X^{-i}) \\
           &amp;= N\times TC(X) - \sum_{i=1}^{N}TC(X^{-i}) \\
           &amp;= TC(X) + DTC(X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
<li><p><strong>Returns</strong></p></li>
<li><p><strong>-------</strong></p></li>
<li><p><strong>float</strong> – The expected S-information.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate
Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.local_o_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">local_o_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#local_o_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.local_o_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local O-information for each sample using Gaussian estimation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\omega(x) &amp;= (2-N)tc(x) + \sum_{i=1}^{N}tc(x^{-i}) \\
           &amp;= tc(x) - dtc(x)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If unspecified it is computed directly from the data.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The series of local O-informations.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating].</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Scagliarini, T., Marinazzo, D., Guo, Y., Stramaglia, S., &amp; Rosas, F. E. (2022).
Quantifying high-order interdependencies on individual patterns via the local O-information: Theory and applications to music analysis.
Physical Review Research, 4(1), 013184.
<a class="reference external" href="https://doi.org/10.1103/PhysRevResearch.4.013184">https://doi.org/10.1103/PhysRevResearch.4.013184</a></p>
<p>Pope, M., Varley, T. F., Grazia Puxeddu, M., Faskowitz, J., &amp; Sporns, O. (2025).
Time-varying synergy/redundancy dominance in the human cerebral cortex. Journal of Physics: Complexity, 6(1), 015015.
<a class="reference external" href="https://doi.org/10.1088/2632-072X/adbaa9">https://doi.org/10.1088/2632-072X/adbaa9</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.o_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">o_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#o_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.o_information" title="Link to this definition">¶</a></dt>
<dd><p>Compute O-information using Gaussian estimation.
O-information quantifies the balance between redundancy (positive values) and synergy (negative values) in multivariate information.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Omega(X) &amp;= (2-N)TC(X) + \sum_{i=1}^{N}TC(X^{-i}) \\
           &amp;= TC(X) - DTC(X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the distribution.
If unspecified it is computed directly from the data.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The expected O-information.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.tse_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">tse_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#tse_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.tse_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Tononi-Sporns-Edelman complexity using Gaussian
estimators.</p>
<div class="math notranslate nohighlight">
\[\begin{split}TSE(X) &amp;= \sum_{k=1}^{\lfloor N/2\rfloor} \bigg\langle I(X^{k}_j;X^{-k}_j) \bigg\rangle_{j} \\
       &amp;= \sum_{k=2}^{N}\bigg[\bigg(\frac{k}{N}\bigg)TC(X) - \langle TC(X^{k}_{j}) \rangle_{j}  \bigg]\end{split}\]</div>
<p>Runtimes scale very badly with system size (as it requires brute-forcing) all possible bipartitions of the system. If the system is too large, a sub-sampling approach is taken: at each scale, num_samples are drawn from the space of bipartitions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_samples</strong> (<em>int</em>) – The number of sample subsets to compute.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The TSE complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Tononi, G., Sporns, O., &amp; Edelman, G. M. (1994).
A measure for brain complexity: Relating functional segregation and integration in the nervous system.
Proceedings of the National Academy of Sciences, 91(11), Article 11.
<a class="reference external" href="https://doi.org/10.1073/pnas.91.11.5033">https://doi.org/10.1073/pnas.91.11.5033</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.description_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">description_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(-1,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/multivariate_mi.html#description_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.description_complexity" title="Link to this definition">¶</a></dt>
<dd><div class="math notranslate nohighlight">
\[C(X) = \frac{DTC(X)}{N}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
<li><p><strong>idxs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The specific subset of variables to compute the total correlation of.
Defaults to computing the TC of the entire covariance matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The expected description complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.partial_entropy_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">partial_entropy_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/decompositions.html#partial_entropy_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.partial_entropy_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>Computes the partial entropy decomposition of a joint distribution
with up to four elements. Uses the Gaussian hmin estimator. See:</p>
<blockquote>
<div><p>Finn, C., &amp; Lizier, J. T. (2020).
Generalised Measures of Multivariate Information Content.
Entropy, 22(2), Article 2.
<a class="reference external" href="https://doi.org/10.3390/e22020216">https://doi.org/10.3390/e22020216</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the elements to analyze.</p></li>
<li><p><strong>data</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple[dict, dict]</p>
</dd>
</dl>
<dl class="simple">
<dt>data<span class="classifier">NDArray[np.floating]</span></dt><dd><p>The numpy array, assumed to be in channels x time format.</p>
</dd>
<dt>cov<span class="classifier">NDArray[np.floating], optional</span></dt><dd><p>The covariance matrix. If none is not provided, it is computed
from the data directly.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The dictionary of local values for each partial entropy atom.
The local values are represented by a numpy array of the same
length as the data array.</p></li>
<li><p><strong>avg</strong> (<em>dict</em>) – The dictionary of expected values for each partial entropy atom.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>)</p></li>
<li><p><strong>data</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, dict]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.partial_information_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">partial_information_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/decompositions.html#partial_information_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.partial_information_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>The pointwise and average partial information decomposition
using the Gaussian imin function. See:</p>
<dl class="simple">
<dt>See:</dt><dd><p>Finn, C., &amp; Lizier, J. T. (2018).
Pointwise Partial Information Decomposition Using the Specificity and Ambiguity Lattices.
Entropy, 20(4), Article 4.
<a class="reference external" href="https://doi.org/10.3390/e20040297">https://doi.org/10.3390/e20040297</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the input variables.</p></li>
<li><p><strong>target</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the target variable(s).</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x time format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix of the data.
The default is COV_NULL.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise PID for every frame in the data.</p></li>
<li><p><strong>avg</strong> (<em>dict</em>) – The average PID for the data</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, dict]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.generalized_information_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">generalized_information_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_posterior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_prior</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/decompositions.html#generalized_information_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.generalized_information_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>Computes the generalized information decomposition from Varley et al.
The GID is a decomposition of the Kullback-Leibler divergence of a
posterior distribution from a prior distribution.</p>
<p>The available redundancy function is the Gaussian hmin.</p>
<dl class="simple">
<dt>See:</dt><dd><p>Varley, T. F. (2024).
Generalized decomposition of multivariate information.
PLOS ONE, 19(2), e0297128.
<a class="reference external" href="https://doi.org/10.1371/journal.pone.0297128">https://doi.org/10.1371/journal.pone.0297128</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the elements to analyze.</p></li>
<li><p><strong>data</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov_posterior</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov_prior</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>tuple[dict, dict]</p>
</dd>
</dl>
<dl class="simple">
<dt>data<span class="classifier">NDArray[np.floating]</span></dt><dd><p>The numpy array, assumed to be in channels x time format.</p>
</dd>
<dt>cov_posterior<span class="classifier">NDArray[np.floating]</span></dt><dd><p>The covariance matrix that defines the prior distribution.</p>
</dd>
<dt>cov_prior<span class="classifier">NDArray[np.floating]</span></dt><dd><p>The covariance matrix that defines the posterior distribution.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The dictionary of local values for each partial entropy atom.
The local values are represented by a numpy array of the same
length as the data array.</p></li>
<li><p><strong>avg</strong> (<em>dict</em>) – The dictionary of expected values for each partial entropy atom.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>)</p></li>
<li><p><strong>data</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov_posterior</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>cov_prior</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, dict]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.representational_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">representational_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">avg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comparator=&lt;built-in</span> <span class="pre">function</span> <span class="pre">min&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/decompositions.html#representational_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.representational_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Computes the representational complexity of a given partial information or entropy lattice.
The representational complexity is a measure of how
much partial information atoms of a given degree of synergy
contribute to the overall mutual information or entropy.</p>
<dl class="simple">
<dt>See:</dt><dd><p>Ehrlich, D. A., Schneider, A. C., Priesemann, V., Wibral, M., &amp; Makkeh, A. (2023).
A Measure of the Complexity of Neural Representations
based on Partial Information Decomposition.
Transactions on Machine Learning Research.
<a class="reference external" href="https://openreview.net/forum?id=R8TU3pfzFr">https://openreview.net/forum?id=R8TU3pfzFr</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>avg</strong> (<em>dict</em>) – The dictionary of partial information/entropy atoms.
Returned from any of the above functions.</p></li>
<li><p><strong>comparator</strong> (<em>function</em><em>, </em><em>optional</em>) – Whether to consider the minimum complexity of an atom.
or the maximum complexity of an atom.
Options are: min, max, np.min, np.max.
The default is min, following the original work
by Ehrlich et al.,.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The representational complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.differential_entropy_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">differential_entropy_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#differential_entropy_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.differential_entropy_rate" title="Link to this definition">¶</a></dt>
<dd><p>Computes the differential entropy rate of a potentially multivariate stochastic process.</p>
<div class="math notranslate nohighlight">
\[H(X)=\frac{1}{4\pi} \int_{-\pi}^{\pi} \log \left( (2\pi e)^N |S_X(\omega)| \right) \, d\omega\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the analysis.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local entropies for each frequency band.</p></li>
<li><p><em>float</em> – The average entropy across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.mutual_information_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">mutual_information_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#mutual_information_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.mutual_information_rate" title="Link to this definition">¶</a></dt>
<dd><p>Computes the mutual information rate between two (potentially multivariate) Gaussian processes.</p>
<div class="math notranslate nohighlight">
\[I(X; Y) = \frac{1}{4\pi} \int_{-\pi}^{\pi} \log \left( \frac{  |S_X(\omega)||S_Y(\omega)| }{ |S_{XY}(\omega)| } \right) d\omega\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the inputs.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the target.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local mutual informations for each frequency band.</p></li>
<li><p><em>float</em> – The average mutual information across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Faes, L., Sparacino, L., Mijatovic, G., Antonacci, Y., Ricci, L., Marinazzo, D., &amp; Stramaglia, S. (2025).
Partial Information Rate Decomposition (No. arXiv:2502.04550). arXiv.
https://doi.org/10.48550/arXiv.2502.04550</p>
<p>Faes, L., Pernice, R., Mijatovic, G., Antonacci, Y., Krohova, J. C., Javorka, M., &amp; Porta, A. (2021).
Information decomposition in the frequency domain: A new framework to study cardiovascular and cardiorespiratory oscillations.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 379(2212), 20200250.
<a class="reference external" href="https://doi.org/10.1098/rsta.2020.0250">https://doi.org/10.1098/rsta.2020.0250</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.total_correlation_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">total_correlation_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#total_correlation_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.total_correlation_rate" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward extension of the mutual information rate to the total correlation.</p>
<div class="math notranslate nohighlight">
\[TC(X,Y,\ldots,Z) = \frac{1}{4\pi} \int_{-\pi}^{\pi} \log \left( \frac{  |S_X(\omega)||S_Y(\omega)|\ldots|S_Z(\omega)| }{ |S_{XY\ldots Z}(\omega)| } \right) d\omega\]</div>
<p>WARNING: As far as I know this TC rate idea has never been formally explored before. It should work fine as a natural generalization of the MI, but it hasn’t ever been published or peer reviewed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the total correlation calculation.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local total correlation  for each frequency band.</p></li>
<li><p><em>float</em> – The average total correlation across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.dual_total_correlation_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">dual_total_correlation_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#dual_total_correlation_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.dual_total_correlation_rate" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward extension of the dual total correlation rate from the total correlation rate.</p>
<p>The dual total correlation is given alternately by:</p>
<div class="math notranslate nohighlight">
\[DTC(X) = H(X) - \sum_{i=1}^{N}H(X_i|X^{-i})\]</div>
<div class="math notranslate nohighlight">
\[DTC(X) = (N-1)TC(X) - \sum_{i=1}^{N}TC(X^{-i})\]</div>
<p>WARNING: As far as I know this rate idea has never been formally explored before. It should work fine as a natural generalization of the MI, but it hasn’t even been published or peer reviewed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the calculation.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local dual total correlation rate for each frequency band.</p></li>
<li><p><em>float</em> – The average dual total correlation across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Abdallah, S. A., &amp; Plumbley, M. D. (2012).
A measure of statistical complexity based on predictive information with application to finite spin systems.
Physics Letters A, 376(4), 275–281.
<a class="reference external" href="https://doi.org/10.1016/j.physleta.2011.10.066">https://doi.org/10.1016/j.physleta.2011.10.066</a></p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.s_information_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">s_information_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#s_information_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.s_information_rate" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward extension of the S-information rate from the total correlation rate.</p>
<p>The S-information is equivalant to:</p>
<div class="math notranslate nohighlight">
\[\Sigma(X) = \sum_{i=1}^{N}I(X_i;X^{-i})\]</div>
<div class="math notranslate nohighlight">
\[\Sigma(X) = TC(X) + DTC(X)\]</div>
<p>WARNING: As far as I know this rate idea has never been formally explored before. It should work fine as a natural generalization of the MI, but it hasn’t even been published or peer reviewed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the calculation.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local S-information rate for each frequency band.</p></li>
<li><p><em>float</em> – The average S-information across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.o_information_rate">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.</span></span><span class="sig-name descname"><span class="pre">o_information_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nperseg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/temporal.html#o_information_rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.o_information_rate" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward extension of the O-information rate from the total correlation rate.</p>
<div class="math notranslate nohighlight">
\[\Omega(X) = (2-N)TC(X) + \sum_{i=1}^{N}TC(X^{-i})\]</div>
<div class="math notranslate nohighlight">
\[\Omega(X) = TC(X) - DTC(X)\]</div>
<p>WARNING: As far as I know this rate idea has never been formally explored before. It should work fine as a natural generalization of the MI, but it hasn’t even been published or peer reviewed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The indices of the channels to include in the calculation.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>fs</strong> (<em>int</em>) – The sampling rate of the time series data in Hz.
The default is 1.</p></li>
<li><p><strong>nperseg</strong> (<em>int</em>) – The number of samples to include in each segment.
Passed to the various scipy.signal functions that compute the spectral analyses.
The default is 1024</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>NDArray[np.floating]</em> – The local O-information rate for each frequency band.</p></li>
<li><p><em>float</em> – The average O-information across the whole spectrum.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[<em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]], float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">¶</a></h2>
</section>
<section id="module-syntropy.gaussian.optimization">
<span id="syntropy-gaussian-optimization-module"></span><h2>syntropy.gaussian.optimization module<a class="headerlink" href="#module-syntropy.gaussian.optimization" title="Link to this heading">¶</a></h2>
<p>Created on Sun Feb 16 13:01:12 2025</p>
<p>&#64;author: thosvarley</p>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.optimization.neg_o_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.optimization.</span></span><span class="sig-name descname"><span class="pre">neg_o_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/optimization.html#neg_o_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.optimization.neg_o_information" title="Link to this definition">¶</a></dt>
<dd><p>A utility function for computing the negative of the O-information
for a hill-climbing optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>tuple</em><em>[</em><em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>set</em><em>[</em><em>int</em><em>]</em><em>]</em>) – A tuple of the arguments to the O-information function.
The covariance matrix (NDArray) and indices (tuple of ints)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The negative O-information.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.optimization.simulated_annealing">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.optimization.</span></span><span class="sig-name descname"><span class="pre">simulated_annealing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cooling_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iters_per_temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convergence_window</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convergence_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/optimization.html#simulated_annealing"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.optimization.simulated_annealing" title="Link to this definition">¶</a></dt>
<dd><p>Implements a simulated annealing algorithm for optimizing
multivariate information measures (O-info, DTC, etc) from
a covariance matrix. Modified from;</p>
<blockquote>
<div><p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems
of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the full distribution.</p></li>
<li><p><strong>function</strong> (<em>function</em>) – The objective function. Should be taken from the
syntropy.gaussian.multivariate_mi module.</p></li>
<li><p><strong>size</strong> (<em>int</em>) – The size of the ensemble of elements.</p></li>
<li><p><strong>temperature</strong> (<em>float</em><em>, </em><em>optional</em>) – The initial temperature to initialize the optimization with.
The default is 1.0.</p></li>
<li><p><strong>cooling_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – The rate at which the annealing cools.
Temperature, T, decreases with T(t+1) = T(t)*cooling_rate
The default is 0.999.</p></li>
<li><p><strong>min_temperature</strong> (<em>float</em><em>, </em><em>optional</em>) – The stopping temperature. The default is 1e-3.</p></li>
<li><p><strong>iters_per_temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – How many times to iterate the annealing algorithim before cooling one step.
The default is 10.</p></li>
<li><p><strong>convergence_window</strong> (<em>int</em><em>, </em><em>optional</em>) – How many steps back in time to consider for assessing convergence.
The default is 100.</p></li>
<li><p><strong>convergence_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – The standard deviation of the window below which convergence is said
to have been achieved.. The default is 1e-6.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>set[int]</em> – The indicies of the optimal set.</p></li>
<li><p><em>float</em> – The value of the optimal set.</p></li>
<li><p><em>list</em> – The time series of values.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[set[int], float, <em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.optimization.irreducible_synergy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.optimization.</span></span><span class="sig-name descname"><span class="pre">irreducible_synergy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/optimization.html#irreducible_synergy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.optimization.irreducible_synergy" title="Link to this definition">¶</a></dt>
<dd><p>Computes whether it is possible to remove an element from a
synergistic sysem and increase the synergy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The covariance matrix that defines the distribution.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em>) – The specific elements to test.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Returns True if the synergy is irreducible (i.e. there is no element that, when removed, increaes the synergy).
Returns False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-syntropy.gaussian.utils">
<span id="syntropy-gaussian-utils-module"></span><h2>syntropy.gaussian.utils module<a class="headerlink" href="#module-syntropy.gaussian.utils" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.check_cov">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">check_cov</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#check_cov"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.check_cov" title="Link to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
<li><p><strong>data</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>ndarray</em>[tuple[int, …], <em>dtype</em>[<em>floating</em>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.make_powerset">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">make_powerset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iterable</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#make_powerset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.make_powerset" title="Link to this definition">¶</a></dt>
<dd><p>Computes the powerset of a collection of elements.</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(\{X_1,X_2,X_3\}) \to (\{\}, \{X_1\}, \{X_2\}, \{X_3\}, \{X_1,X_2\}, \{X_1,X_3\}, \{X_1,X_2,X_3\} )\]</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.unpack_atom">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">unpack_atom</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#unpack_atom"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.unpack_atom" title="Link to this definition">¶</a></dt>
<dd><p>A utitlity function to unpack tuples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>atom</strong> (<em>tuple</em>)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>set</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.local_precompute_sources">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">local_precompute_sources</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#local_precompute_sources"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.local_precompute_sources" title="Link to this definition">¶</a></dt>
<dd><p>A utility function that pre-computes the local entropies
of every subset of the data. This speeds up the PID/GID/PED
by orders of magnitude.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix of the data.
The default is COV_NULL.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary of the local entropies for every source.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.hmin_differential_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">hmin_differential_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sources</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#hmin_differential_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.hmin_differential_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>For a collection of sources <span class="math notranslate nohighlight">\(\alpha=\{a_1, a_2, \ldots, a_k\}\)</span>, computes
the redundnat entropy shared by all sources as:</p>
<p><span class="math notranslate nohighlight">\(h_{\cap}^{min}(\alpha) = \min_{i}(a_{i})\)</span></p>
<dl class="simple">
<dt>See:</dt><dd><p>Finn, C., &amp; Lizier, J. T. (2020).
Generalised Measures of Multivariate Information Content.
Entropy, 22(2), Article 2.
<a class="reference external" href="https://doi.org/10.3390/e22020216">https://doi.org/10.3390/e22020216</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial information atom. In the form <span class="math notranslate nohighlight">\(((a_1,),(a_2,)\ldots)\)</span>.</p></li>
<li><p><strong>sources</strong> (<em>dict</em>) – The pre-computed collection of sources.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data, assumed to be in sources x samples format.</p></li>
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local redundancies for each sample.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.imin_differential_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">imin_differential_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sources</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#imin_differential_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.imin_differential_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the differential redundnacy between a partial information atom <span class="math notranslate nohighlight">\(\alpha=\{a_i,\ldots,a_k\}\)</span> and a (potentially multivariate) target. Uses a Gaussian estimator for the local entropies.</p>
<p><span class="math notranslate nohighlight">\(i_{\cap}^{min}(\alpha;t) = \min_{i}h(a_i) - \min_{i}h(a_i|t)\)</span></p>
<p>This is NOT the I_min from Williams and Beer - it is the local redundancy function from Finn and Lizier.</p>
<dl class="simple">
<dt>See:</dt><dd><p>Finn, C., &amp; Lizier, J. T. (2018).
Pointwise Partial Information Decomposition Using
the Specificity and Ambiguity Lattices.
Entropy, 20(4), Article 4.
<a class="reference external" href="https://doi.org/10.3390/e20040297">https://doi.org/10.3390/e20040297</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial information atom. In the form <span class="math notranslate nohighlight">\(((a_1,),(a_2,)\ldots)\)</span>.</p></li>
<li><p><strong>sources</strong> (<em>dict</em>) – The pre-computed collection of sources.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em>) – The indicies of the inputs - one index per element of the tuple.</p></li>
<li><p><strong>target</strong> (<em>tuple</em>) – The (potentially multivariate) indices of the collective target.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data, assumed to be in channels x samples format.</p></li>
<li><p><strong>cov</strong> (<em>ndarray</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>dtype</em><em>[</em><em>floating</em><em>]</em><em>]</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local differnetial redundancy for each frame.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.mobius_inversion">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">mobius_inversion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decomposition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(None,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([[-1.]])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#mobius_inversion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.mobius_inversion" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Mobius inversion on a lattice, given a redundancy function.</p>
<dl class="simple">
<dt>See:</dt><dd><p>Williams, P. L., &amp; Beer, R. D. (2010).
Nonnegative Decomposition of Multivariate Information.
arXiv:1004.2515 [Math-Ph, Physics:Physics, q-Bio].
http://arxiv.org/abs/1004.2515</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decomposition</strong> (<em>str</em>) – Whehter to do a PID or PED. Options: “pid”, “ped”.</p></li>
<li><p><strong>data</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – The data, assumed to be in channels x samples format.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em>) – The variables to be decomposed.</p></li>
<li><p><strong>target</strong> (<em>tuple</em><em>, </em><em>optional</em>) – If doing a PID, the indices of the target.
If doing a PED, leave false.
The default is (None,).</p></li>
<li><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em><em>, </em><em>optional</em>) – The covariance matrix that defines the multivairate distribution.
If left empty, it is computed from the data. The default is COV_NULL.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise values of each atom in the decomposition for each sample.</p></li>
<li><p><strong>avg</strong> (<em>dict</em>) – The expected values of each atom in the decomposition.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, dict]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.gaussian.utils.correlation_to_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.gaussian.utils.</span></span><span class="sig-name descname"><span class="pre">correlation_to_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cov</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/gaussian/utils.html#correlation_to_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.gaussian.utils.correlation_to_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Converts a Pearson correlation matrix to a Guassian mutual
information matrix based on the identity:</p>
<p><span class="math notranslate nohighlight">\(I(X;Y) = \frac{-\log(1-(r_{XY})^2)}{2}\)</span></p>
<p>Where <span class="math notranslate nohighlight">\(r_{XY}\)</span> is the Pearson correlation coefficient between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Also works for a covariance matrix if the processes have 0 mean
and unit variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cov</strong> (<em>NDArray</em><em>[</em><em>np.floating</em><em>]</em>) – A covariance matrix.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The equivalent mutual information matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>NDArray[np.floating]</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Syntropy</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="syntropy.discrete.html">syntropy.discrete package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">syntropy.gaussian package</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntropy.knn.html">syntropy.knn package</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntropy.neural.html">syntropy.neural package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="syntropy.discrete.html" title="previous chapter">syntropy.discrete package</a></li>
      <li>Next: <a href="syntropy.knn.html" title="next chapter">syntropy.knn package</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Thomas F. Varley.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/api/syntropy.gaussian.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>