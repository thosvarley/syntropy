<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>syntropy.discrete package &#8212; Syntropy 1.0.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="syntropy.gaussian package" href="syntropy.gaussian.html" />
    <link rel="prev" title="Quickstart Guide" href="../quickstart.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-syntropy.discrete">
<span id="syntropy-discrete-package"></span><h1>syntropy.discrete package<a class="headerlink" href="#module-syntropy.discrete" title="Link to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.shannon_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">shannon_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/shannon.html#shannon_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.shannon_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Shannon entropy of the distribution <span class="math notranslate nohighlight">\(P(x)\)</span>.</p>
<div class="math notranslate nohighlight">
\[H(X) = -\sum_{x\in\mathcal{X}} P(x) \log P(x)\]</div>
<p>To compute the entropy of a subset of the variables in the joint
distribution, use the <code class="xref py py-func docutils literal notranslate"><span class="pre">syntropy.discrete.utils.get_marginals()</span></code> function from the utils
library.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise entropy for each state in the joint distribution.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average entropy</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.conditional_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">conditional_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/shannon.html#conditional_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.conditional_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the conditional entropy of X given Y.</p>
<div class="math notranslate nohighlight">
\[H(X|Y) = H(X,Y) - H(Y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the variables to compute the entropy on.</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indicies of the variables to contintue on.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em>) – <dl class="simple">
<dt>DESCRIPTION.joint_distribution<span class="classifier">dict</span></dt><dd><p>The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise entropy for each state in the joint distribution.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average entropy</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/shannon.html#mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the mutual information between X and Y.</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X;Y) &amp;= H(X) + H(Y) - H(X,Y) \\
       &amp;= H(X) - H(X|Y) \\
       &amp;= H(Y) - H(Y|X) \\
       &amp;= H(X,Y) - H(X|Y) - H(Y|X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the X variable(s).</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the Y variable(s).</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise mutual information for each state in the joint distribution.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average mutual information</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.conditional_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">conditional_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">idxs_z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/shannon.html#conditional_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.conditional_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the mutual information between X and Y condioned on Z.</p>
<div class="math notranslate nohighlight">
\[\begin{split}I(X,Y|Z) &amp;= H(X|Z) + H(Y|Z) - H(X,Y|Z) \\
         &amp;= I(X;Y,Z) - I(X;Z)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs_x</strong> (<em>tuple</em>) – The indices of the X variable(s).</p></li>
<li><p><strong>idxs_y</strong> (<em>tuple</em>) – The indices of the Y variable(s).</p></li>
<li><p><strong>idxs_z</strong> (<em>tuple</em>) – The indices of the variables to condition on.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise mutual information for each state in the joint distribution.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average mutual information</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.kullback_leibler_divergence">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">kullback_leibler_divergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">posterior_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/shannon.html#kullback_leibler_divergence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.kullback_leibler_divergence" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Kullback-Leibler divergence from a prior distribution P(X) and
and posterior distribution Q(X).</p>
<div class="math notranslate nohighlight">
\[D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>posterior_distribution</strong> (<em>dict</em>) – The joint distribution of the posterior distribution P(X).</p></li>
<li><p><strong>prior_distribution</strong> (<em>dict</em>) – The joint distribution of the prior distribution Q(X)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – The pointwise Kullback-Leibler divergence for each state in the joint distribution.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average Kullback-Leibler divergence.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[dict, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>Computes the average and pointwise total correlations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}TC(X) &amp;= D_{KL}(P(X) || \prod_{i=1}^{N}P(X_i) \\
      &amp;= \sum_{i=1}^{N}H(X_i) - H(X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>]</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise TC .</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average TC.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Watanabe, S. (1960). Information Theoretical Analysis of Multivariate Correlation.
IBM Journal of Research and Development, 4(1), Article 1.
<a class="reference external" href="https://doi.org/10.1147/rd.41.0066">https://doi.org/10.1147/rd.41.0066</a></p>
<p>Tononi, G., Sporns, O., &amp; Edelman, G. M. (1994).
A measure for brain complexity: Relating functional segregation and integration in the nervous system.
Proceedings of the National Academy of Sciences, 91(11), Article 11.
<a class="reference external" href="https://doi.org/10.1073/pnas.91.11.5033">https://doi.org/10.1073/pnas.91.11.5033</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.dual_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">dual_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#dual_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.dual_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local and expected dual total correlations for the joint distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}DTC(X) &amp;= H(X) - \sum_{i=1}^{N}H(X_i|X^{-i}) \\
       &amp;= (N-1)\times TC(X) - \sum_{i=1}^{N}TC(X^{-i})\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise DTC .</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average DTC.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Abdallah, S. A., &amp; Plumbley, M. D. (2012).
A measure of statistical complexity based on predictive information with application to finite spin systems.
Physics Letters A, 376(4), 275–281.
<a class="reference external" href="https://doi.org/10.1016/j.physleta.2011.10.066">https://doi.org/10.1016/j.physleta.2011.10.066</a></p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.s_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">s_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#s_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.s_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local and expected S-information for the joint distribution.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Sigma(X) &amp;= \sum_{i=1}^{N}I(X_i;X^{-i}) \\
           &amp;= N\times TC(X) - \sum_{i=1}^{N}TC(X^{-i}) \\
           &amp;= TC(X) + DTC(X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise S-information .</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average S-information.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.o_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">o_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#o_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.o_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local and expected O-informations for the joint distribution.
O-information quantifies the balance between redundancy (positive values) and synergy (negative values) in multivariate information.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Omega(X) &amp;= (2-N)TC(X) + \sum_{i=1}^{N}TC(X^{-i}) \\
           &amp;= TC(X) - DTC(X)\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise O-information .</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average O-information.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Rosas, F., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019).
Quantifying High-order Interdependencies via Multivariate Extensions of the Mutual Information.
Physical Review E, 100(3), Article 3.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.tse_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">tse_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#tse_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.tse_complexity" title="Link to this definition">¶</a></dt>
<dd><p>The Tononi-Sporns-Edelman neural complexity measure, which provides a measure of the balance between integration and segregation across scales.</p>
<div class="math notranslate nohighlight">
\[\begin{split}TSE(X) &amp;= \sum_{k=1}^{\lfloor N/2\rfloor} \bigg\langle I(X^{k}_j;X^{-k}_j) \bigg\rangle_{j} \\
       &amp;= \sum_{k=2}^{N}\bigg[\bigg(\frac{k}{N}\bigg)TC(X) - \langle TC(X^{k}_{j}) \rangle_{j}  \bigg]\end{split}\]</div>
<p>Runtimes scale very badly with system size (as it requires brute-forcing) all possible bipartitions of the system. If the system is too large, a sub-sampling approach is taken: at each scale, num_samples are drawn from the space of bipartitions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em>) – The number of samples to do for each subset size..</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The TSE complexity. No local complexity is computed. .</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Tononi, G., Sporns, O., &amp; Edelman, G. M. (1994).
A measure for brain complexity: Relating functional segregation and integration in the nervous system.
Proceedings of the National Academy of Sciences, 91(11), Article 11.
<a class="reference external" href="https://doi.org/10.1073/pnas.91.11.5033">https://doi.org/10.1073/pnas.91.11.5033</a></p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.description_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">description_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#description_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.description_complexity" title="Link to this definition">¶</a></dt>
<dd><p>The description complexity was proposed by Tononi and Sporns as a
heuristic, easy-to-compute approximation of the full TSE-Complexity.
Later shown by Varley et al., to be directly proportional to
the dual total correlation.</p>
<div class="math notranslate nohighlight">
\[C(X) = \frac{DTC(X)}{N}\]</div>
<p>Where <span class="math notranslate nohighlight">\(N\)</span> is the number of elements in <span class="math notranslate nohighlight">\(X\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise description complexity.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average description complexity.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Varley, T. F., Pope, M., Faskowitz, J., &amp; Sporns, O. (2023).
Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex.
Communications Biology, 6(1), Article 1.
<a class="reference external" href="https://doi.org/10.1038/s42003-023-04843-w">https://doi.org/10.1038/s42003-023-04843-w</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.co_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">co_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#co_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.co_information" title="Link to this definition">¶</a></dt>
<dd><p>Computes the cO-information, the third generalization of bivariate mutual information. Unlike total correlation and dual total correlation, the cO-information can be negative and is difficult to interpret.</p>
<div class="math notranslate nohighlight">
\[Co(X) = \sum_{\xi\subseteq X}(-1)^{|\xi|}H(\xi)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, float]</em>) – The pointwise cO-information.</p></li>
<li><p><strong>avg</strong> (<em>float</em>) – The average cO-information.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(tuple[dict, float], &lt;class ‘float’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Bell, A. J. (2003, April).
The Co-information lattice.
4th International Symposium on Independent Component Analysis and
Blind Signal Separation, Nara, Japan.
<a class="reference external" href="https://www.semanticscholar.org/paper/THE-CO-INFORMATION-LATTICE-Bell/25a0cd8d486d5ffd204485685226f189e6eadd4d">https://www.semanticscholar.org/paper/THE-CO-INFORMATION-LATTICE-Bell/25a0cd8d486d5ffd204485685226f189e6eadd4d</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.connected_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">connected_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximum_order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/multivariate_mi.html#connected_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.connected_information" title="Link to this definition">¶</a></dt>
<dd><p>Returns the connected information profile from Schneidman et al.,
which decomposes the total correlation into contributing parts of
different orders:</p>
<div class="math notranslate nohighlight">
\[TC(X) = \sum_{k=2}^{N}TC^{k}(X)\]</div>
<p>Where the <cite>k</cite> superscript refers to the maximum-entropy distribution that preserves all marginals of order <cite>k</cite>.</p>
<p>One of the few measures that can reliably distinguish between the
JAMES_DYADIC and JAMES_TRIADIC distributions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>[</em><em>int</em><em>, </em><em>...</em><em>]</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
<li><p><strong>maximum_order</strong> (<em>int</em><em>, </em><em>optional</em>) – The highest order of marginals to sweep.
The default sweeps all.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The connected information profile.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Schneidman, E., Still, S., Berry, M. J., &amp; Bialek, W. (2003).
Network Information and Connected Correlations.
Physical Review Letters, 91(23), 238701.
<a class="reference external" href="https://doi.org/10.1103/PhysRevLett.91.238701">https://doi.org/10.1103/PhysRevLett.91.238701</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.partial_entropy_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">partial_entropy_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">redundancy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/decompositions.html#partial_entropy_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.partial_entropy_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>Computes the partial entropy decomposition of a joint distribution
with up to four elements.</p>
<p>The available redundancy functions are h_min from Finn and Lizier
and h_sx from Varley et al.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_{\min}(\alpha) &amp;= \min(\alpha_i) \\
   h_{sx}(\alpha) &amp;= \log\frac{1}{P(\alpha_1\cup ... \cup\alpha_N)}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>redundancy</strong> (<em>str</em>) – The redundnacy function to use.
“hmin” for the measure from Finn and Lizier,
“hsx” for the measure from Varley et al.,</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The joint distribution object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, dict]</em>) – A dictionary of dictionaries,
The outer dictionary has one key for each joint state.
Each inner dictionary is the lookup of partial entropy atoms.</p></li>
<li><p><strong>avg</strong> (<em>dict[tuple, float]</em>) – The expected value for each partial entropy atom.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(&lt;class ‘dict’&gt;, &lt;class ‘dict’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Finn, C., &amp; Lizier, J. T. (2020).
Generalised Measures of Multivariate Information Content.
Entropy, 22(2), Article 2.
<a class="reference external" href="https://doi.org/10.3390/e22020216">https://doi.org/10.3390/e22020216</a></p>
<p>Varley, T. F., Pope, M., Maria Grazia, P., Joshua, F., &amp; Sporns, O. (2023).
Partial entropy decomposition reveals higher-order
information structures in human brain activity.
Proceedings of the National Academy of Sciences,
120(30), e2300888120.
<a class="reference external" href="https://doi.org/10.1073/pnas.2300888120">https://doi.org/10.1073/pnas.2300888120</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.partial_information_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">partial_information_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">redundancy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/decompositions.html#partial_information_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.partial_information_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>Computes the partial information decomposition for up to four input variables
onto one (potentially joint) target variable.</p>
<p>The available redundancy functions are <span class="math notranslate nohighlight">\(i_{\min}\)</span> from Finn and Lizier and <span class="math notranslate nohighlight">\(i_{sx}\)</span> from Makkeh et al..</p>
<div class="math notranslate nohighlight">
\[\begin{split}i_{\min}(\alpha;t) &amp;= \min h(\alpha_i) - \min h(\alpha_i|t) \\
   i_{sx}(\alpha;t) &amp;= \log\frac{P(t)-P(t\cap(\alpha_1\cup ...\cup\alpha_k))}{1-P(\bar\alpha_1\cap ...\cap\alpha_N)}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>redundancy</strong> (<em>str</em>) – The redundancy function.
“imin” for the Finn and Lizier measure
“isx” for the Makkeh et al., measure</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em>) – The set of up to four input elements.</p></li>
<li><p><strong>target</strong> (<em>tuple</em>) – The set of target elements.
If len(target) &gt; 1, then the target is the joint
state of all elements</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The joint distribution object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict</em>) – A dictionary of dictionaries,
The outer dictionary has one key for each joint state.
Each inner dictionary is the lookup of partial information atoms.</p></li>
<li><p><strong>avg</strong> (<em>dict</em>) – The expected value for each partial information atom.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(&lt;class ‘dict’&gt;, &lt;class ‘dict’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Williams, P. L., &amp; Beer, R. D. (2010).
Nonnegative Decomposition of Multivariate Information.
arXiv:1004.2515 [Math-Ph, Physics:Physics, q-Bio].
http://arxiv.org/abs/1004.2515</p>
<p>Finn, C., &amp; Lizier, J. T. (2018).
Pointwise Partial Information Decomposition Using
the Specificity and Ambiguity Lattices.
Entropy, 20(4), Article 4.
<a class="reference external" href="https://doi.org/10.3390/e20040297">https://doi.org/10.3390/e20040297</a></p>
<p>Makkeh, A., Gutknecht, A. J., &amp; Wibral, M. (2021).
Introducing a differentiable measure of pointwise
shared information.
Physical Review E, 103(3), 032149.
<a class="reference external" href="https://doi.org/10.1103/PhysRevE.103.032149">https://doi.org/10.1103/PhysRevE.103.032149</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.generalized_information_decomposition">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">generalized_information_decomposition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">redundancy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posterior_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/decompositions.html#generalized_information_decomposition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.generalized_information_decomposition" title="Link to this definition">¶</a></dt>
<dd><p>Computes the generalized information decomposition from Varley et al.
The GID is a decomposition of the Kullback-Leibler divergence of a
posterior distribution from a prior distribution.</p>
<p>Available redundnacy functions are “hmin” and “hsx”. See
the documentation for the partial_entropy_decomposition() function
for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>redundancy</strong> (<em>str</em>) – The localizable redundancy function.
Options are: hmin and hsx.</p></li>
<li><p><strong>posterior_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The posterior distribution.
The support set of this distribution must be a subset of
the supppirt set of the prior distribution.</p></li>
<li><p><strong>prior_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The prior distribution.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>ptw</strong> (<em>dict[tuple, dict]</em>) – A dict of dicts.
The local Kullback-Leibler divergence for each atom for each state.</p></li>
<li><p><strong>avg</strong> (<em>dict[tuple, float]</em>) – The average Kullback-Leibler divergence for each atom.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(&lt;class ‘dict’&gt;, &lt;class ‘dict’&gt;)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Varley, T. F. (2024).
Generalized decomposition of multivariate information.
PLOS ONE, 19(2), e0297128.
<a class="reference external" href="https://doi.org/10.1371/journal.pone.0297128">https://doi.org/10.1371/journal.pone.0297128</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.representational_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">representational_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">avg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comparator=&lt;built-in</span> <span class="pre">function</span> <span class="pre">min&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/decompositions.html#representational_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.representational_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Computes the representational complexity of a given partial information or entropy lattice.
The representational complexity is a measure of how
much partial information atoms of a given degree of synergy
contribute to the overall mutual information or entropy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>avg</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The dictionary of partial information/entropy atoms.
Returned from any of the above functions.</p></li>
<li><p><strong>comparator</strong> (<em>function</em><em>, </em><em>optional</em>) – Whether to consider the minimum complexity of an atom.
or the maximum complexity of an atom.
Options are: min, max, np.min, np.max.
The default is min, following the original work
by Ehrlich et al.,.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The representational complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Ehrlich, D. A., Schneider, A. C., Priesemann, V., Wibral, M., &amp; Makkeh, A. (2023).
A Measure of the Complexity of Neural Representations
based on Partial Information Decomposition.
Transactions on Machine Learning Research.
<a class="reference external" href="https://openreview.net/forum?id=R8TU3pfzFr">https://openreview.net/forum?id=R8TU3pfzFr</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.lempel_ziv_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dictionary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.lempel_ziv_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Uses the classic Lempel-Ziv compression algorithm to estimate the entropy rate of a one-dimensional array with <span class="math notranslate nohighlight">\(N\)</span> samples.
If each element in X is a multi-dimensional tuple, then the result is equivalent to the joint entropy rate.</p>
<p>The extension to multivariate Lempel-Ziv is straightforward and involves representing the joint state of each element at time t as a tuple (X(t), Y(t)) and treating the two sources as a single joint source.</p>
<p>Here, the dictionary length <span class="math notranslate nohighlight">\(|D|\)</span> is normalized:</p>
<div class="math notranslate nohighlight">
\[\textnormal{Complexity}(X) = \frac{|D|\log|D|}{N}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>list</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
<li><p><strong>return_substrings</strong> (<em>bool</em><em>, </em><em>optional</em>) – DESCRIPTION. The default is False.</p></li>
<li><p><strong>return_dictionary</strong> (<em>bool</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>float</em> – The estimated Lempel-Ziv complexity.</p></li>
<li><p><em>set</em> – The dictionary (only returned if return_dictionary == True)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float | tuple[float, set]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Schartner, M. M., Carhart-Harris, R. L., Barrett, A. B., Seth, A. K., &amp; Muthukumaraswamy, S. D. (2017).
Increased spontaneous MEG signal diversity for psychoactive doses of ketamine, LSD and psilocybin.
Scientific Reports, 7, 46421.
<a class="reference external" href="https://doi.org/10.1038/srep46421">https://doi.org/10.1038/srep46421</a></p>
<p>Blanc, J.-L., Schmidt, N., Bonnier, L., Pezard, L., &amp; Lesne, A. (2008).
Quantifying Neural Correlations Using Lempel-Ziv Complexity.
Deuxième conférence française de Neurosciences Computationnelles,
Marseille, France.
<a class="reference external" href="https://hal.science/hal-00331599/document">https://hal.science/hal-00331599/document</a></p>
<p>Zozor, S., Ravier, P., &amp; Buttelli, O. (2005).
On Lempel–Ziv complexity for multidimensional data analysis.
Physica A: Statistical Mechanics and Its Applications, 345(1), 285–302.
<a class="reference external" href="https://doi.org/10.1016/j.physa.2004.07.025">https://doi.org/10.1016/j.physa.2004.07.025</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.lempel_ziv_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.lempel_ziv_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Estimates the discrete mutual information rate for two channels X and Y with the Lempel-Ziv compression algorithm.
This measure can be transiently negative, although in the limit it approximates the discrete information rate.</p>
<div class="math notranslate nohighlight">
\[I_{LZ}(X;Y) = LZ(X) + LZ(Y) - LZ(X,Y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
<li><p><strong>Y</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The estimated mutual information rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Zozor, S., Ravier, P., &amp; Buttelli, O. (2005).
On Lempel–Ziv complexity for multidimensional data analysis.
Physica A: Statistical Mechanics and Its Applications, 345(1), 285–302.
<a class="reference external" href="https://doi.org/10.1016/j.physa.2004.07.025">https://doi.org/10.1016/j.physa.2004.07.025</a></p>
<p>Blanc, J.-L., Schmidt, N., Bonnier, L., Pezard, L., &amp; Lesne, A. (2008).
Quantifying Neural Correlations Using Lempel-Ziv Complexity.
Deuxième conférence française de Neurosciences Computationnelles,
Marseille, France.
<a class="reference external" href="https://hal.science/hal-00331599/document">https://hal.science/hal-00331599/document</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.lempel_ziv_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.lempel_ziv_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward generalization of the mutual information rate given by
Zozor et al., and Blanc et al.,</p>
<div class="math notranslate nohighlight">
\[TC_{LZ}(X) = \sum_{i=1}^{N} LZ(X_i) - LZ(X)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>np.ndarray</em>) – A multi-dimensional discrete array, assumed to be in
channels x time format.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The estimated total correlation rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.cross_lempel_ziv_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.</span></span><span class="sig-name descname"><span class="pre">cross_lempel_ziv_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#cross_lempel_ziv_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.cross_lempel_ziv_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Lempel-Ziv complexity of a string X
using a pre-constructed dictionary optimized on Y. The relative
Lempel-Ziv complexity is the extra patterns that appear in X but
not in the compressed Y.</p>
<p>This code uses the optimized dictionary for Y, rather than searching all
substrings of Y. Using all possible substrings is very impractical for
long time series. As a result, however, it is very sensitive to the
particular temporal ordering of fluctuations in X and Y.</p>
<p>Should only be used when X and Y were recorded at the same time, such
as channels in an EEG/fMRI/MEG recording.</p>
<p>MATHEMATICAL GARUNTEES GIVEN IN ZIV &amp; MERHAV 1993 ARE NOT PRESERVED
BY THIS METHOD. THIS FUNCTION REMAINS EXPERIMENTAL</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
<li><p><strong>Y</strong> (<em>np.ndarray</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>complexity</strong> – The Lempel-ziv relative complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Ziv, J., &amp; Merhav, N. (1993).
A Measure of Relative Entropy between Individual Sequences with
Application to Universal Classification.
Proceedings. IEEE International Symposium on Information Theory, 352–352.
<a class="reference external" href="https://doi.org/10.1109/ISIT.1993.748668">https://doi.org/10.1109/ISIT.1993.748668</a></p>
</dd></dl>

<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">¶</a></h2>
</section>
<section id="module-syntropy.discrete.alpha_synergy">
<span id="syntropy-discrete-alpha-synergy-module"></span><h2>syntropy.discrete.alpha_synergy module<a class="headerlink" href="#module-syntropy.discrete.alpha_synergy" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.alpha_synergy.alpha_synergistic_entropy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.alpha_synergy.</span></span><span class="sig-name descname"><span class="pre">alpha_synergistic_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">definition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'min'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/alpha_synergy.html#alpha_synergistic_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.alpha_synergy.alpha_synergistic_entropy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the <span class="math notranslate nohighlight">\(\alpha\)</span>-synergistic entropy for a joint distribution
for a given value of <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="math notranslate nohighlight">
\[h^{syn}_{\alpha}(x) = \min_{a\subseteq x, |a|=\alpha} h(x^{a}|x^{-a})\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint distribution dictionary object.</p></li>
<li><p><strong>alpha</strong> (<em>int</em>) – The scale to consider.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of samples to trial. The default is -1, in which case, all permutations are trialed.</p></li>
<li><p><strong>definition</strong> (<em>str</em><em>, </em><em>optional</em>) – How to define the loss of information. Can be “min”, “max”, or “avg”. The default is “min”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local alpha-synergy for each state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple, float]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Varley, T. F. (2024).
A scalable synergy-first backbone decomposition of
higher-order structures in complex systems.
Npj Complexity, 1(1), 1–11.
<a class="reference external" href="https://doi.org/10.1038/s44260-024-00011-1">https://doi.org/10.1038/s44260-024-00011-1</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.alpha_synergy.partial_entropy_spectra">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.alpha_synergy.</span></span><span class="sig-name descname"><span class="pre">partial_entropy_spectra</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">definition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'min'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/alpha_synergy.html#partial_entropy_spectra"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.alpha_synergy.partial_entropy_spectra" title="Link to this definition">¶</a></dt>
<dd><p>Computes the partial synergy for every value of <span class="math notranslate nohighlight">\(alpha\)</span> (the spectrum) for each local state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The joint probability dictionary object.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of samples to trial. The default is -1, in which case, all permutations are trialed.</p></li>
<li><p><strong>definition</strong> (<em>str</em><em>, </em><em>optional</em>) – How to define the loss of information. Can be “min”, “max”, or “avg”. The default is “min”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The alpha-synergistic entropy spectrum for each state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple,list]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.alpha_synergy.partial_kullback_leibler_spectra">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.alpha_synergy.</span></span><span class="sig-name descname"><span class="pre">partial_kullback_leibler_spectra</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">posterior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">definition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'min'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/alpha_synergy.html#partial_kullback_leibler_spectra"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.alpha_synergy.partial_kullback_leibler_spectra" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local Kullback-Leibler spectrum for each state</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>posterior</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The distribution that describes the posterior beliefs.</p></li>
<li><p><strong>prior</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The distribution that describes the prior beliefs.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of samples to trial.
The default is -1, in which case, all permutations are trialed.</p></li>
<li><p><strong>definition</strong> (<em>str</em><em>, </em><em>optional</em>) – How to define the loss of information.
Can be “min”, “max”, or “avg”. The default is “min”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The alpha-synergistic DKL spectrum for each state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple,list]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.alpha_synergy.partial_total_correlation_spectra">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.alpha_synergy.</span></span><span class="sig-name descname"><span class="pre">partial_total_correlation_spectra</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">definition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'min'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/alpha_synergy.html#partial_total_correlation_spectra"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.alpha_synergy.partial_total_correlation_spectra" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local total correlation spectrum for each state using the Kullback-Leibler divergence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The joint probability dictionary object.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of samples to trial. The default is -1, in which case, all permutations are trialed.</p></li>
<li><p><strong>definition</strong> (<em>str</em><em>, </em><em>optional</em>) – How to define the loss of information. Can be “min”, “max”, or “avg”. The default is “min”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The alpha-synergistic total correlation spectrum for each state.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple,list]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.alpha_synergy.partial_information_spectra">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.alpha_synergy.</span></span><span class="sig-name descname"><span class="pre">partial_information_spectra</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">definition</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'min'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/alpha_synergy.html#partial_information_spectra"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.alpha_synergy.partial_information_spectra" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local mutual information spectrum for each state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tuple</em>) – The indices of the input variables.</p></li>
<li><p><strong>target</strong> (<em>tuple</em>) – The indices of the target variables.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>,</em><em>float</em><em>]</em>) – The joint probability dictionary object.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of samples to trial. The default is -1, in which case, all permutations are trialed.</p></li>
<li><p><strong>definition</strong> (<em>str</em><em>, </em><em>optional</em>) – How to define the loss of information. Can be “min”, “max”, or “avg”. The default is “min”.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>list[float]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-syntropy.discrete.optimization">
<span id="syntropy-discrete-optimization-module"></span><h2>syntropy.discrete.optimization module<a class="headerlink" href="#module-syntropy.discrete.optimization" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.optimization.constrained_maximum_entropy_distributions">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.optimization.</span></span><span class="sig-name descname"><span class="pre">constrained_maximum_entropy_distributions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">marginal_constraints</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[(None,)]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/optimization.html#constrained_maximum_entropy_distributions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.optimization.constrained_maximum_entropy_distributions" title="Link to this definition">¶</a></dt>
<dd><p>Uses the iterated proportional fitting (IPF) algorithm to find
the maximum-entropy distribution consistant with given constraints.</p>
<p>If the marginal constrains are given, uses those.
If order is given, finds the maximum entropy distribution consistant
with all marginals of the given order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>joint_distribution</strong> (<em>dict</em>) – The joint distribution from which to compute the marginal constraints.</p></li>
<li><p><strong>marginal_constraints</strong> (<em>list</em><em>, </em><em>optional</em>) – A list of tuples: each tuple corresponds to a set of marginals to constrain. The default is (None,).</p></li>
<li><p><strong>order</strong> (<em>int</em><em>, </em><em>optional</em>) – The order of marginals to fix. The default is -1.</p></li>
<li><p><strong>max_iters</strong> (<em>int</em><em>, </em><em>optional</em>) – The maximum number of iterations the algorithm can run for. The default is 10_000.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>optional</em>) – The value below which further refinements of the maximum entropy distribution are stopped.
The default is 1e-6.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The optimized distribution consistent with the given marginal constraints.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple, float]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-syntropy.discrete.temporal">
<span id="syntropy-discrete-temporal-module"></span><h2>syntropy.discrete.temporal module<a class="headerlink" href="#module-syntropy.discrete.temporal" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.temporal.lempel_ziv_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.temporal.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dictionary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.temporal.lempel_ziv_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Uses the classic Lempel-Ziv compression algorithm to estimate the entropy rate of a one-dimensional array with <span class="math notranslate nohighlight">\(N\)</span> samples.
If each element in X is a multi-dimensional tuple, then the result is equivalent to the joint entropy rate.</p>
<p>The extension to multivariate Lempel-Ziv is straightforward and involves representing the joint state of each element at time t as a tuple (X(t), Y(t)) and treating the two sources as a single joint source.</p>
<p>Here, the dictionary length <span class="math notranslate nohighlight">\(|D|\)</span> is normalized:</p>
<div class="math notranslate nohighlight">
\[\textnormal{Complexity}(X) = \frac{|D|\log|D|}{N}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>list</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
<li><p><strong>return_substrings</strong> (<em>bool</em><em>, </em><em>optional</em>) – DESCRIPTION. The default is False.</p></li>
<li><p><strong>return_dictionary</strong> (<em>bool</em>)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>float</em> – The estimated Lempel-Ziv complexity.</p></li>
<li><p><em>set</em> – The dictionary (only returned if return_dictionary == True)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float | tuple[float, set]</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Schartner, M. M., Carhart-Harris, R. L., Barrett, A. B., Seth, A. K., &amp; Muthukumaraswamy, S. D. (2017).
Increased spontaneous MEG signal diversity for psychoactive doses of ketamine, LSD and psilocybin.
Scientific Reports, 7, 46421.
<a class="reference external" href="https://doi.org/10.1038/srep46421">https://doi.org/10.1038/srep46421</a></p>
<p>Blanc, J.-L., Schmidt, N., Bonnier, L., Pezard, L., &amp; Lesne, A. (2008).
Quantifying Neural Correlations Using Lempel-Ziv Complexity.
Deuxième conférence française de Neurosciences Computationnelles,
Marseille, France.
<a class="reference external" href="https://hal.science/hal-00331599/document">https://hal.science/hal-00331599/document</a></p>
<p>Zozor, S., Ravier, P., &amp; Buttelli, O. (2005).
On Lempel–Ziv complexity for multidimensional data analysis.
Physica A: Statistical Mechanics and Its Applications, 345(1), 285–302.
<a class="reference external" href="https://doi.org/10.1016/j.physa.2004.07.025">https://doi.org/10.1016/j.physa.2004.07.025</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.temporal.lempel_ziv_mutual_information">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.temporal.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_mutual_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_mutual_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.temporal.lempel_ziv_mutual_information" title="Link to this definition">¶</a></dt>
<dd><p>Estimates the discrete mutual information rate for two channels X and Y with the Lempel-Ziv compression algorithm.
This measure can be transiently negative, although in the limit it approximates the discrete information rate.</p>
<div class="math notranslate nohighlight">
\[I_{LZ}(X;Y) = LZ(X) + LZ(Y) - LZ(X,Y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
<li><p><strong>Y</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.)..</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The estimated mutual information rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Zozor, S., Ravier, P., &amp; Buttelli, O. (2005).
On Lempel–Ziv complexity for multidimensional data analysis.
Physica A: Statistical Mechanics and Its Applications, 345(1), 285–302.
<a class="reference external" href="https://doi.org/10.1016/j.physa.2004.07.025">https://doi.org/10.1016/j.physa.2004.07.025</a></p>
<p>Blanc, J.-L., Schmidt, N., Bonnier, L., Pezard, L., &amp; Lesne, A. (2008).
Quantifying Neural Correlations Using Lempel-Ziv Complexity.
Deuxième conférence française de Neurosciences Computationnelles,
Marseille, France.
<a class="reference external" href="https://hal.science/hal-00331599/document">https://hal.science/hal-00331599/document</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.temporal.conditional_lempel_ziv_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.temporal.</span></span><span class="sig-name descname"><span class="pre">conditional_lempel_ziv_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#conditional_lempel_ziv_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.temporal.conditional_lempel_ziv_complexity" title="Link to this definition">¶</a></dt>
<dd><p>The conditional entropy rate estimated with the Lempel-Ziv algorithm.</p>
<div class="math notranslate nohighlight">
\[LZ(X|Y) = LZ(X,Y) - LZ(Y)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
<li><p><strong>Y</strong> (<em>list</em>) – A discrete list. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The estimated conditional entropy rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Zozor, S., Ravier, P., &amp; Buttelli, O. (2005).
On Lempel–Ziv complexity for multidimensional data analysis.
Physica A: Statistical Mechanics and Its Applications, 345(1), 285–302.
<a class="reference external" href="https://doi.org/10.1016/j.physa.2004.07.025">https://doi.org/10.1016/j.physa.2004.07.025</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.temporal.lempel_ziv_total_correlation">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.temporal.</span></span><span class="sig-name descname"><span class="pre">lempel_ziv_total_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#lempel_ziv_total_correlation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.temporal.lempel_ziv_total_correlation" title="Link to this definition">¶</a></dt>
<dd><p>A straightforward generalization of the mutual information rate given by
Zozor et al., and Blanc et al.,</p>
<div class="math notranslate nohighlight">
\[TC_{LZ}(X) = \sum_{i=1}^{N} LZ(X_i) - LZ(X)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>np.ndarray</em>) – A multi-dimensional discrete array, assumed to be in
channels x time format.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The estimated total correlation rate.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.temporal.cross_lempel_ziv_complexity">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.temporal.</span></span><span class="sig-name descname"><span class="pre">cross_lempel_ziv_complexity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/temporal.html#cross_lempel_ziv_complexity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.temporal.cross_lempel_ziv_complexity" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Lempel-Ziv complexity of a string X
using a pre-constructed dictionary optimized on Y. The relative
Lempel-Ziv complexity is the extra patterns that appear in X but
not in the compressed Y.</p>
<p>This code uses the optimized dictionary for Y, rather than searching all
substrings of Y. Using all possible substrings is very impractical for
long time series. As a result, however, it is very sensitive to the
particular temporal ordering of fluctuations in X and Y.</p>
<p>Should only be used when X and Y were recorded at the same time, such
as channels in an EEG/fMRI/MEG recording.</p>
<p>MATHEMATICAL GARUNTEES GIVEN IN ZIV &amp; MERHAV 1993 ARE NOT PRESERVED
BY THIS METHOD. THIS FUNCTION REMAINS EXPERIMENTAL</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>np.ndarray</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
<li><p><strong>Y</strong> (<em>np.ndarray</em>) – A discrete array. Can contain digits 0-9 and/or
single-character strings (“A”, “a”, “B”, “b”, etc.).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>complexity</strong> – The Lempel-ziv relative complexity.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Ziv, J., &amp; Merhav, N. (1993).
A Measure of Relative Entropy between Individual Sequences with
Application to Universal Classification.
Proceedings. IEEE International Symposium on Information Theory, 352–352.
<a class="reference external" href="https://doi.org/10.1109/ISIT.1993.748668">https://doi.org/10.1109/ISIT.1993.748668</a></p>
</dd></dl>

</section>
<section id="module-syntropy.discrete.utils">
<span id="syntropy-discrete-utils-module"></span><h2>syntropy.discrete.utils module<a class="headerlink" href="#module-syntropy.discrete.utils" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.make_powerset">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">make_powerset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iterable</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#make_powerset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.make_powerset" title="Link to this definition">¶</a></dt>
<dd><p>A utility function for quickly making powersets,</p>
<p>powerset([1,2,3]) –&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.clean_distribution">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">clean_distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#clean_distribution"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.clean_distribution" title="Link to this definition">¶</a></dt>
<dd><p>A utility function to remove states with 0 probability</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The joint probability distribution with zero-probability elements removed.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.reduce_state">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">reduce_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">source</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#reduce_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.reduce_state" title="Link to this definition">¶</a></dt>
<dd><p>A utility function for reducing tuples
to just the elements in the source.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<em>tuple</em>) – The particular state of each variable.</p></li>
<li><p><strong>source</strong> (<em>tuple</em>) – The indices of the variable to remove.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The reduced state consisting only of those
elements indexed in the source variable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.construct_joint_distribution">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">construct_joint_distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#construct_joint_distribution"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.construct_joint_distribution" title="Link to this definition">¶</a></dt>
<dd><p>Given a channels x time, discrete Numpy array, computes
the probability distribution that describes the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>np.ndarray</em>) – The data: assumed to be in elements x time format.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.get_marginal_distribution">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">get_marginal_distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#get_marginal_distribution"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.get_marginal_distribution" title="Link to this definition">¶</a></dt>
<dd><p>Returns the marginal distribution of the variables
indexed by the idxs tuple. The opposite of the
marginalize_out() function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em>) – The indices of the variable to retain.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The marginal joint probability distribution object.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.marginalize_out">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">marginalize_out</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idxs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#marginalize_out"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.marginalize_out" title="Link to this definition">¶</a></dt>
<dd><p>Returns a distribution with the variables indexed by
idxs marginalized out.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>idxs</strong> (<em>tuple</em>) – The indices of the variables to be marginalized out.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A joint probability distribution dictionary.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.get_all_marginal_distributions">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">get_all_marginal_distributions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#get_all_marginal_distributions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.get_all_marginal_distributions" title="Link to this definition">¶</a></dt>
<dd><p>Computes the set of all marginal probability distributions.
If the original distribution has variables:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(P(X_1, X_2, X_3)\)</span></p>
</div></blockquote>
<dl class="simple">
<dt>Returns a dictionary of dictionaries for each:</dt><dd><p><span class="math notranslate nohighlight">\(P(X_1,), P(X_2,), P(X_3,), P(X_1, X_2), P(X_1, X_3), P(X_2, X_3), P(X_1,X_2,X_3)\)</span></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary of dictionaries: each key is a set of marginals,
each value is the associated marginal distribution .</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict[tuple, dict]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.local_precompute_sources">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">local_precompute_sources</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#local_precompute_sources"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.local_precompute_sources" title="Link to this definition">¶</a></dt>
<dd><p>A utility function that computes the local entropy of each subset of
elements. This speeds up the computation using the hmin function
considerably,</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>For each state, the joint entropy of every subset.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.hmin_discrete_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">hmin_discrete_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sources</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#hmin_discrete_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.hmin_discrete_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>For a collection of sources <span class="math notranslate nohighlight">\(\alpha=\{a_1, a_2, \ldots, a_k\}\)</span>, computes
the redundnat entropy shared by all sources as:</p>
<p><span class="math notranslate nohighlight">\(h_{\cap}^{min}(\alpha) = \min_{i}h(a_i)\)</span></p>
<dl class="simple">
<dt>See:</dt><dd><p>Finn, C., &amp; Lizier, J. T. (2020).
Generalised Measures of Multivariate Information Content.
Entropy, 22(2), Article 2.
<a class="reference external" href="https://doi.org/10.3390/e22020216">https://doi.org/10.3390/e22020216</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial entropy atom.</p></li>
<li><p><strong>state</strong> (<em>tuple</em>) – The state of the system.</p></li>
<li><p><strong>sources</strong> (<em>dict</em>) – The pre-computed local entropies constructed by the
precompute_local_entropies() function.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The redundant entropy shared by every source in the atom.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.imin_discrete_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">imin_discrete_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sources</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#imin_discrete_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.imin_discrete_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>For a collection of sources <span class="math notranslate nohighlight">\(\alpha = \{a_{1}, a_{2}, \ldots, a_{k}\}\)</span> and a target <span class="math notranslate nohighlight">\(t\)</span> the redundancy is defined as:</p>
<p><span class="math notranslate nohighlight">\(i_{min}(\alpha;t) = \min_{i}h(a_i) - \min_{i}h(a_i|t)\)</span></p>
<dl class="simple">
<dt>See:</dt><dd><p>Finn, C., &amp; Lizier, J. T. (2020).
Generalised Measures of Multivariate Information Content.
Entropy, 22(2), Article 2.
<a class="reference external" href="https://doi.org/10.3390/e22020216">https://doi.org/10.3390/e22020216</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial information atom.</p></li>
<li><p><strong>state</strong> (<em>tuple</em>) – The joint-state of the systems.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em>) – The indices of the input variables.</p></li>
<li><p><strong>target</strong> (<em>tuple</em>) – The indices of the target variable. May be multivariate.</p></li>
<li><p><strong>sources</strong> (<em>dict</em>) – The pre-computed local entropies
for the joint state of the source and the target.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The local redundant mutual information.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.hsx_discrete_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">hsx_discrete_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#hsx_discrete_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.hsx_discrete_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>Computes the redundant entropy shared by a set of sources using the <span class="math notranslate nohighlight">\(h_{sx}\)</span> function.</p>
<p>For a collection of sources <span class="math notranslate nohighlight">\(\alpha = \{a_{1}, a_{2}, \ldots, a_{k}\}\)</span>,
the redundancy is defined as</p>
<p><span class="math notranslate nohighlight">\(h^{sx}_{\cap}(\alpha) = -\log_{2} P(a_{1} \cup a_{2} \cup \ldots \cup a_{k})\)</span>.</p>
<dl class="simple">
<dt>See:</dt><dd><p>Varley, T. F., Pope, M., Maria Grazia, P., Joshua, F., &amp; Sporns, O. (2023).
Partial entropy decomposition reveals higher-order information structures in human brain activity.
Proceedings of the National Academy of Sciences, 120(30), e2300888120.
<a class="reference external" href="https://doi.org/10.1073/pnas.2300888120">https://doi.org/10.1073/pnas.2300888120</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial entropy atom.</p></li>
<li><p><strong>state</strong> (<em>tuple</em>) – The state of the system.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The redundant entropy shared by every source in the atom..</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.isx_discrete_redundancy">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">isx_discrete_redundancy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">atom</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#isx_discrete_redundancy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.isx_discrete_redundancy" title="Link to this definition">¶</a></dt>
<dd><p>For a collection of sources <span class="math notranslate nohighlight">\(\alpha = \{a_{1}, a_{2}, \ldots, a_{k}\}\)</span> and a target <span class="math notranslate nohighlight">\(t\)</span> the redundancy is defined as:</p>
<p><span class="math notranslate nohighlight">\(i_{sx}(\alpha;t) = \log_{2}\frac{P(t) - P(t \cap (\bar{a}_1 \cap \ldots \cap \bar{a}_k)}{1 - P(\bar{a}_1 \cap \ldots \cap \bar{a}_{k})} - \log_2 P(t)\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>atom</strong> (<em>tuple</em>) – The partial information atom.</p></li>
<li><p><strong>state</strong> (<em>tuple</em>) – The joint-state of the systems.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em>) – The indices of the input variables.</p></li>
<li><p><strong>target</strong> (<em>tuple</em>) – The indices of the target variable. May be multivariate.</p></li>
<li><p><strong>sources</strong> (<em>dict</em>) – The pre-computed local entropies
for the joint state of the source and the target.</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The redundant mutual informations.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="syntropy.discrete.utils.mobius_inversion">
<span class="sig-prename descclassname"><span class="pre">syntropy.discrete.utils.</span></span><span class="sig-name descname"><span class="pre">mobius_inversion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decomposition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">joint_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">redundancy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(None,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(None,)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/syntropy/discrete/utils.html#mobius_inversion"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#syntropy.discrete.utils.mobius_inversion" title="Link to this definition">¶</a></dt>
<dd><p>Computes the Mobius inversion on the antichain lattice.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decomposition</strong> (<em>str</em>) – <dl class="simple">
<dt>Which decomposition to use:</dt><dd><p>For partial information decomposition use “pid”.
For partial entropy decomposition use “ped”.</p>
</dd>
</dl>
</p></li>
<li><p><strong>joint_distribution</strong> (<em>dict</em><em>[</em><em>tuple</em><em>, </em><em>float</em><em>]</em>) – The joint probability distribution.
Keys are tuples corresponding to the state of each element.
The valules are the probabilities.</p></li>
<li><p><strong>redundancy</strong> (<em>str</em>) – The redundancy function.</p></li>
<li><p><strong>inputs</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The indicies of the inputs. The default is (None,)</p></li>
<li><p><strong>target</strong> (<em>tuple</em><em>, </em><em>optional</em>) – The (potentially multivariate) indices of the target.
The default is (None,).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The pointwise and average partial information atom dictionaries.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>(dict, dict)</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Syntropy</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">syntropy.discrete package</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntropy.gaussian.html">syntropy.gaussian package</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntropy.knn.html">syntropy.knn package</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntropy.neural.html">syntropy.neural package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../quickstart.html" title="previous chapter">Quickstart Guide</a></li>
      <li>Next: <a href="syntropy.gaussian.html" title="next chapter">syntropy.gaussian package</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Thomas F. Varley.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/api/syntropy.discrete.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>